Triage Agent: The first line of defense. Its job is to ingest raw data (logs, alerts), filter out the noise, and group related events.

Investigation Agent: Receives high-priority alerts from the Triage Agent. This agent's primary role is deep-dive analysis.

Threat Hunting Agent: Proactively searches for threats that may have evaded initial detection, guided by hypotheses and threat intelligence.

Response Agent: Takes action based on the findings of the Investigation and Threat Hunting agents. Its actions can range from isolating a compromised device to drafting a detailed incident report.


THIS ONE WITH ORIGINAL HYPERPARAM

--- Training Logistic Regression on Balanced Data ---
d:\VII\ANT\.venv\Lib\site-packages\sklearn\linear_model\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):
STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT

Increase the number of iterations to improve the convergence (max_iter=1000).
You might also want to scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(
--- Evaluating Logistic Regression on Original Test Data ---

Accuracy for Logistic Regression: 0.5733
Classification Report for Logistic Regression:
                          precision    recall  f1-score   support

                  Benign       0.99      0.45      0.62   1215429
                     Bot       0.62      1.00      0.76     57238
        DDOS attack-HOIC       0.97      1.00      0.99    137203
        DoS attacks-Hulk       0.98      1.00      0.99     92382
DoS attacks-SlowHTTPTest       0.64      0.55      0.59     27978
          FTP-BruteForce       0.71      0.78      0.74     38671
           Infilteration       0.04      0.77      0.07     32128
          SSH-Bruteforce       0.97      1.00      0.99     37518

                accuracy                           0.57   1638547
               macro avg       0.74      0.82      0.72   1638547
            weighted avg       0.94      0.57      0.67   1638547


--- Training Random Forest on Balanced Data ---
--- Evaluating Random Forest on Original Test Data ---

Accuracy for Random Forest: 0.9074
Classification Report for Random Forest:
...
            weighted avg       0.96      0.91      0.93   1638547


--- Training XGBoost on Balanced Data ---
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
d:\VII\ANT\.venv\Lib\site-packages\xgboost\training.py:183: UserWarning: [15:54:53] WARNING: C:\actions-runner\_work\xgboost\xgboost\src\learner.cc:738: 
Parameters: { "use_label_encoder" } are not used.

  bst.update(dtrain, iteration=i, fobj=obj)
--- Evaluating XGBoost on Original Test Data ---

Accuracy for XGBoost: 0.8000
Classification Report for XGBoost:
                          precision    recall  f1-score   support

                  Benign       0.99      0.75      0.85   1215429
                     Bot       1.00      1.00      1.00     57238
        DDOS attack-HOIC       1.00      1.00      1.00    137203
        DoS attacks-Hulk       1.00      1.00      1.00     92382
DoS attacks-SlowHTTPTest       0.76      0.52      0.62     27978
          FTP-BruteForce       0.72      0.88      0.79     38671
           Infilteration       0.08      0.78      0.14     32128
          SSH-Bruteforce       1.00      1.00      1.00     37518

                accuracy                           0.80   1638547
               macro avg       0.82      0.87      0.80   1638547
            weighted avg       0.97      0.80      0.86   1638547


--- Training LightGBM on Balanced Data ---
[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.975105 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 7281
...
[LightGBM] [Info] Start training from score -2.079442
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
--- Evaluating LightGBM on Original Test Data ---
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...
d:\VII\ANT\.venv\Lib\site-packages\sklearn\utils\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names
  warnings.warn(

Accuracy for LightGBM: 0.8550
Classification Report for LightGBM:
                          precision    recall  f1-score   support

                  Benign       0.99      0.82      0.90   1215429
                     Bot       1.00      1.00      1.00     57238
        DDOS attack-HOIC       1.00      1.00      1.00    137203
        DoS attacks-Hulk       1.00      1.00      1.00     92382
DoS attacks-SlowHTTPTest       0.93      0.53      0.68     27978
          FTP-BruteForce       0.74      0.97      0.84     38671
           Infilteration       0.11      0.81      0.19     32128
          SSH-Bruteforce       1.00      1.00      1.00     37518

                accuracy                           0.86   1638547
               macro avg       0.85      0.89      0.83   1638547
            weighted avg       0.97      0.86      0.90   1638547


------------------------------------OLD UNSUPERVISED--------------------------------
--- Training Isolation Forest ---
--- Evaluating Isolation Forest ---

Anomaly Detection Report for Isolation Forest:
              precision    recall  f1-score   support

Anomaly (-1)       0.28      0.15      0.20   2115587
  Normal (1)       0.75      0.86      0.80   6077145

    accuracy                           0.68   8192732
   macro avg       0.51      0.51      0.50   8192732
weighted avg       0.62      0.68      0.64   8192732

Confusion Matrix for Isolation Forest:
 [[ 326926 1788661]
 [ 843624 5233521]]

--- Training Local Outlier Factor ---
--- Evaluating Local Outlier Factor ---

Anomaly Detection Report for Local Outlier Factor:
              precision    recall  f1-score   support

Anomaly (-1)       0.70      0.78      0.74   2115587
  Normal (1)       0.92      0.88      0.90   6077145

    accuracy                           0.86   8192732
   macro avg       0.81      0.83      0.82   8192732
weighted avg       0.86      0.86      0.86   8192732

Confusion Matrix for Local Outlier Factor:
 [[1649041  466546]
 [ 706598 5370547]]

--- Training One-Class SVM ---
--- Evaluating One-Class SVM ---

Anomaly Detection Report for One-Class SVM:
              precision    recall  f1-score   support

Anomaly (-1)       0.83      0.15      0.25   2115587
  Normal (1)       0.77      0.99      0.87   6077145

    accuracy                           0.77   8192732
   macro avg       0.80      0.57      0.56   8192732
weighted avg       0.78      0.77      0.71   8192732

Confusion Matrix for One-Class SVM:
 [[ 313284 1802303]
 [  64609 6012536]]

Script finished successfully! ðŸŽ‰



----------2nd one -------------------
--- Training Isolation Forest ---
--- Evaluating Isolation Forest ---
*** New best model found: Isolation Forest with Anomaly F1-Score: 0.2810 ***

Anomaly Detection Report for Isolation Forest:
              precision    recall  f1-score   support

Anomaly (-1)       0.28      0.28      0.28   2115587
  Normal (1)       0.75      0.75      0.75   6077145

    accuracy                           0.63   8192732
   macro avg       0.52      0.52      0.52   8192732
weighted avg       0.63      0.63      0.63   8192732

Confusion Matrix for Isolation Forest:
 [[ 594139 1521448]
 [1519286 4557859]]

--- Training Local Outlier Factor ---
--- Evaluating Local Outlier Factor ---
*** New best model found: Local Outlier Factor with Anomaly F1-Score: 0.6169 ***

Anomaly Detection Report for Local Outlier Factor:
              precision    recall  f1-score   support

Anomaly (-1)       0.51      0.77      0.62   2115587
  Normal (1)       0.90      0.74      0.82   6077145

    accuracy                           0.75   8192732
   macro avg       0.71      0.76      0.72   8192732
weighted avg       0.80      0.75      0.76   8192732

Confusion Matrix for Local Outlier Factor:
 [[1637393  478194]
 [1555680 4521465]]

--- Training One-Class SVM ---
--- Evaluating One-Class SVM ---