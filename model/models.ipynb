{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa4eebc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532a3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "786b4252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Data ---\n",
    "# IMPORTANT: Replace 'your_dataset.csv' with the actual path to your CSE-CIC-IDS2018 dataset file.\n",
    "try:\n",
    "    df = pd.read_csv('../data\\combined_realistic.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Dataset file not found. Please update the file path.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b0d0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to ../data/combined_realistic_clean.csv.gz\n",
      "Shape: (8192732, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Input/output files\n",
    "input_file = \"../data/combined_realistic.csv\"\n",
    "output_file = \"../data/combined_realistic_clean.csv.gz\"\n",
    "\n",
    "# Chunk size (adjust if needed)\n",
    "chunksize = 500_000\n",
    "\n",
    "# List to store cleaned chunks\n",
    "clean_chunks = []\n",
    "\n",
    "# Process file in chunks\n",
    "for chunk in pd.read_csv(input_file, chunksize=chunksize):\n",
    "    # Strip column names\n",
    "    chunk.columns = chunk.columns.str.strip()\n",
    "    \n",
    "    # Replace infinities with NaN\n",
    "    chunk.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # Option 1: Drop rows with any NaN\n",
    "    chunk = chunk.dropna()\n",
    "    \n",
    "    # Option 2 (optional): instead of dropping, fill NaNs with median\n",
    "    # chunk = chunk.fillna(chunk.median(numeric_only=True))\n",
    "    \n",
    "    clean_chunks.append(chunk)\n",
    "\n",
    "# Concatenate all chunks\n",
    "df_clean = pd.concat(clean_chunks, ignore_index=True)\n",
    "\n",
    "# Save to compressed CSV with 2 decimal places\n",
    "df_clean.to_csv(output_file, index=False, float_format=\"%.2f\", compression=\"gzip\")\n",
    "\n",
    "print(f\"Cleaned dataset saved to {output_file}\")\n",
    "print(f\"Shape: {df_clean.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30abf344",
   "metadata": {},
   "source": [
    "#### Filtered Dataset CSV, taken from 30 features that were decided in further cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60cfe80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of top 30 features you identified\n",
    "top_30_features = [\n",
    "    'Init Fwd Win Byts', 'Fwd Seg Size Min', 'Dst Port', 'Fwd Header Len', 'Flow IAT Min', \n",
    "    'Flow Duration', 'Fwd IAT Max', 'Fwd IAT Min', 'Fwd Pkts/s', 'Flow Pkts/s', \n",
    "    'Fwd IAT Tot', 'Flow IAT Max', 'Fwd Pkt Len Mean', 'Fwd IAT Mean', 'Flow IAT Mean', \n",
    "    'Bwd Pkts/s', 'Pkt Len Mean', 'TotLen Fwd Pkts', 'Flow Byts/s', 'Init Bwd Win Byts', \n",
    "    'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Bwd Pkt Len Max', 'Pkt Len Var', \n",
    "    'Fwd Seg Size Avg', 'Tot Fwd Pkts', 'Bwd Header Len', 'Pkt Len Max', \n",
    "    'Subflow Fwd Byts', 'Subflow Fwd Pkts'\n",
    "]\n",
    "\n",
    "# Create the final list of columns to keep\n",
    "columns_to_keep = top_30_features + ['Label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba737600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering the existing DataFrame in memory...\n",
      "New subset DataFrame created successfully.\n",
      "Saving the new dataset to '../data/filtered_dataset.csv'...\n",
      "\n",
      "Script finished successfully! ðŸŽ‰\n",
      "Your new file '../data/filtered_dataset.csv' is ready to use.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Filter the DataFrame ---\n",
    "print(\"Filtering the existing DataFrame in memory...\")\n",
    "\n",
    "# Best practice: Clean column names to avoid errors\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Create the new subset DataFrame by selecting only the columns you need\n",
    "df_subset = df[columns_to_keep]\n",
    "\n",
    "print(\"New subset DataFrame created successfully.\")\n",
    "\n",
    "# --- 3. Save the New, Smaller Dataset ---\n",
    "filtered_filename = '../data/filtered_dataset.csv'\n",
    "print(f\"Saving the new dataset to '{filtered_filename}'...\")\n",
    "\n",
    "# Using index=False prevents pandas from writing an unnecessary row index\n",
    "df_subset.to_csv(filtered_filename, index=False)\n",
    "\n",
    "print(\"\\nScript finished successfully! ðŸŽ‰\")\n",
    "print(f\"Your new file '{filtered_filename}' is ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df7a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8192732 entries, 0 to 8229038\n",
      "Data columns (total 79 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Dst Port           int64  \n",
      " 1   Protocol           int64  \n",
      " 2   Flow Duration      int64  \n",
      " 3   Tot Fwd Pkts       int64  \n",
      " 4   Tot Bwd Pkts       int64  \n",
      " 5   TotLen Fwd Pkts    int64  \n",
      " 6   TotLen Bwd Pkts    float64\n",
      " 7   Fwd Pkt Len Max    int64  \n",
      " 8   Fwd Pkt Len Min    int64  \n",
      " 9   Fwd Pkt Len Mean   float64\n",
      " 10  Fwd Pkt Len Std    float64\n",
      " 11  Bwd Pkt Len Max    int64  \n",
      " 12  Bwd Pkt Len Min    int64  \n",
      " 13  Bwd Pkt Len Mean   float64\n",
      " 14  Bwd Pkt Len Std    float64\n",
      " 15  Flow Byts/s        float64\n",
      " 16  Flow Pkts/s        float64\n",
      " 17  Flow IAT Mean      float64\n",
      " 18  Flow IAT Std       float64\n",
      " 19  Flow IAT Max       float64\n",
      " 20  Flow IAT Min       float64\n",
      " 21  Fwd IAT Tot        float64\n",
      " 22  Fwd IAT Mean       float64\n",
      " 23  Fwd IAT Std        float64\n",
      " 24  Fwd IAT Max        float64\n",
      " 25  Fwd IAT Min        float64\n",
      " 26  Bwd IAT Tot        float64\n",
      " 27  Bwd IAT Mean       float64\n",
      " 28  Bwd IAT Std        float64\n",
      " 29  Bwd IAT Max        float64\n",
      " 30  Bwd IAT Min        float64\n",
      " 31  Fwd PSH Flags      int64  \n",
      " 32  Bwd PSH Flags      int64  \n",
      " 33  Fwd URG Flags      int64  \n",
      " 34  Bwd URG Flags      int64  \n",
      " 35  Fwd Header Len     int64  \n",
      " 36  Bwd Header Len     int64  \n",
      " 37  Fwd Pkts/s         float64\n",
      " 38  Bwd Pkts/s         float64\n",
      " 39  Pkt Len Min        int64  \n",
      " 40  Pkt Len Max        int64  \n",
      " 41  Pkt Len Mean       float64\n",
      " 42  Pkt Len Std        float64\n",
      " 43  Pkt Len Var        float64\n",
      " 44  FIN Flag Cnt       int64  \n",
      " 45  SYN Flag Cnt       int64  \n",
      " 46  RST Flag Cnt       int64  \n",
      " 47  PSH Flag Cnt       int64  \n",
      " 48  ACK Flag Cnt       int64  \n",
      " 49  URG Flag Cnt       int64  \n",
      " 50  CWE Flag Count     int64  \n",
      " 51  ECE Flag Cnt       int64  \n",
      " 52  Down/Up Ratio      int64  \n",
      " 53  Pkt Size Avg       float64\n",
      " 54  Fwd Seg Size Avg   float64\n",
      " 55  Bwd Seg Size Avg   float64\n",
      " 56  Fwd Byts/b Avg     int64  \n",
      " 57  Fwd Pkts/b Avg     int64  \n",
      " 58  Fwd Blk Rate Avg   int64  \n",
      " 59  Bwd Byts/b Avg     int64  \n",
      " 60  Bwd Pkts/b Avg     int64  \n",
      " 61  Bwd Blk Rate Avg   int64  \n",
      " 62  Subflow Fwd Pkts   int64  \n",
      " 63  Subflow Fwd Byts   int64  \n",
      " 64  Subflow Bwd Pkts   int64  \n",
      " 65  Subflow Bwd Byts   int64  \n",
      " 66  Init Fwd Win Byts  int64  \n",
      " 67  Init Bwd Win Byts  int64  \n",
      " 68  Fwd Act Data Pkts  int64  \n",
      " 69  Fwd Seg Size Min   int64  \n",
      " 70  Active Mean        float64\n",
      " 71  Active Std         float64\n",
      " 72  Active Max         float64\n",
      " 73  Active Min         float64\n",
      " 74  Idle Mean          float64\n",
      " 75  Idle Std           float64\n",
      " 76  Idle Max           float64\n",
      " 77  Idle Min           float64\n",
      " 78  Label              object \n",
      "dtypes: float64(37), int64(41), object(1)\n",
      "memory usage: 4.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81c49381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and preprocessing data...\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Data Cleaning & Preprocessing ---\n",
    "print(\"Cleaning and preprocessing data...\")\n",
    "\n",
    "# Drop the timestamp column as it's not a useful feature in its raw format\n",
    "if 'Timestamp' in df.columns:\n",
    "    df = df.drop(columns=['Timestamp'])\n",
    "\n",
    "# Clean column names (strip leading/trailing spaces)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# The dataset is known to have infinity and NaN values.\n",
    "# Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Drop rows with NaN values. For this dataset, this is a safe approach.\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d414524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 8192732 entries, 0 to 8229038\n",
      "Data columns (total 79 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Dst Port           int64  \n",
      " 1   Protocol           int64  \n",
      " 2   Flow Duration      int64  \n",
      " 3   Tot Fwd Pkts       int64  \n",
      " 4   Tot Bwd Pkts       int64  \n",
      " 5   TotLen Fwd Pkts    int64  \n",
      " 6   TotLen Bwd Pkts    float64\n",
      " 7   Fwd Pkt Len Max    int64  \n",
      " 8   Fwd Pkt Len Min    int64  \n",
      " 9   Fwd Pkt Len Mean   float64\n",
      " 10  Fwd Pkt Len Std    float64\n",
      " 11  Bwd Pkt Len Max    int64  \n",
      " 12  Bwd Pkt Len Min    int64  \n",
      " 13  Bwd Pkt Len Mean   float64\n",
      " 14  Bwd Pkt Len Std    float64\n",
      " 15  Flow Byts/s        float64\n",
      " 16  Flow Pkts/s        float64\n",
      " 17  Flow IAT Mean      float64\n",
      " 18  Flow IAT Std       float64\n",
      " 19  Flow IAT Max       float64\n",
      " 20  Flow IAT Min       float64\n",
      " 21  Fwd IAT Tot        float64\n",
      " 22  Fwd IAT Mean       float64\n",
      " 23  Fwd IAT Std        float64\n",
      " 24  Fwd IAT Max        float64\n",
      " 25  Fwd IAT Min        float64\n",
      " 26  Bwd IAT Tot        float64\n",
      " 27  Bwd IAT Mean       float64\n",
      " 28  Bwd IAT Std        float64\n",
      " 29  Bwd IAT Max        float64\n",
      " 30  Bwd IAT Min        float64\n",
      " 31  Fwd PSH Flags      int64  \n",
      " 32  Bwd PSH Flags      int64  \n",
      " 33  Fwd URG Flags      int64  \n",
      " 34  Bwd URG Flags      int64  \n",
      " 35  Fwd Header Len     int64  \n",
      " 36  Bwd Header Len     int64  \n",
      " 37  Fwd Pkts/s         float64\n",
      " 38  Bwd Pkts/s         float64\n",
      " 39  Pkt Len Min        int64  \n",
      " 40  Pkt Len Max        int64  \n",
      " 41  Pkt Len Mean       float64\n",
      " 42  Pkt Len Std        float64\n",
      " 43  Pkt Len Var        float64\n",
      " 44  FIN Flag Cnt       int64  \n",
      " 45  SYN Flag Cnt       int64  \n",
      " 46  RST Flag Cnt       int64  \n",
      " 47  PSH Flag Cnt       int64  \n",
      " 48  ACK Flag Cnt       int64  \n",
      " 49  URG Flag Cnt       int64  \n",
      " 50  CWE Flag Count     int64  \n",
      " 51  ECE Flag Cnt       int64  \n",
      " 52  Down/Up Ratio      int64  \n",
      " 53  Pkt Size Avg       float64\n",
      " 54  Fwd Seg Size Avg   float64\n",
      " 55  Bwd Seg Size Avg   float64\n",
      " 56  Fwd Byts/b Avg     int64  \n",
      " 57  Fwd Pkts/b Avg     int64  \n",
      " 58  Fwd Blk Rate Avg   int64  \n",
      " 59  Bwd Byts/b Avg     int64  \n",
      " 60  Bwd Pkts/b Avg     int64  \n",
      " 61  Bwd Blk Rate Avg   int64  \n",
      " 62  Subflow Fwd Pkts   int64  \n",
      " 63  Subflow Fwd Byts   int64  \n",
      " 64  Subflow Bwd Pkts   int64  \n",
      " 65  Subflow Bwd Byts   int64  \n",
      " 66  Init Fwd Win Byts  int64  \n",
      " 67  Init Bwd Win Byts  int64  \n",
      " 68  Fwd Act Data Pkts  int64  \n",
      " 69  Fwd Seg Size Min   int64  \n",
      " 70  Active Mean        float64\n",
      " 71  Active Std         float64\n",
      " 72  Active Max         float64\n",
      " 73  Active Min         float64\n",
      " 74  Idle Mean          float64\n",
      " 75  Idle Std           float64\n",
      " 76  Idle Max           float64\n",
      " 77  Idle Min           float64\n",
      " 78  Label              object \n",
      "dtypes: float64(37), int64(41), object(1)\n",
      "memory usage: 4.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5820a",
   "metadata": {},
   "source": [
    "#### Checking on how the Flags are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edc92e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"FIN Flag Cnt\",\n",
    "    \"SYN Flag Cnt\",\n",
    "    \"RST Flag Cnt\",\n",
    "    \"PSH Flag Cnt\",\n",
    "    \"ACK Flag Cnt\",\n",
    "    \"URG Flag Cnt\",\n",
    "    \"CWE Flag Count\",\n",
    "    \"ECE Flag Cnt\"\n",
    "]\n",
    "\n",
    "df_flags = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1187ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIN Flag Cnt</th>\n",
       "      <th>SYN Flag Cnt</th>\n",
       "      <th>RST Flag Cnt</th>\n",
       "      <th>PSH Flag Cnt</th>\n",
       "      <th>ACK Flag Cnt</th>\n",
       "      <th>URG Flag Cnt</th>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <th>ECE Flag Cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229034</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229035</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229036</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229037</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229038</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8192732 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         FIN Flag Cnt  SYN Flag Cnt  RST Flag Cnt  PSH Flag Cnt  ACK Flag Cnt  \\\n",
       "0                   0             0             0             0             0   \n",
       "1                   0             0             0             0             0   \n",
       "2                   0             0             0             0             0   \n",
       "3                   0             0             0             1             0   \n",
       "4                   0             0             0             1             0   \n",
       "...               ...           ...           ...           ...           ...   \n",
       "8229034             0             0             1             1             0   \n",
       "8229035             0             0             1             1             0   \n",
       "8229036             0             0             1             1             0   \n",
       "8229037             0             0             1             1             0   \n",
       "8229038             0             0             1             1             0   \n",
       "\n",
       "         URG Flag Cnt  CWE Flag Count  ECE Flag Cnt  \n",
       "0                   0               0             0  \n",
       "1                   0               0             0  \n",
       "2                   0               0             0  \n",
       "3                   0               0             0  \n",
       "4                   0               0             0  \n",
       "...               ...             ...           ...  \n",
       "8229034             0               0             1  \n",
       "8229035             0               0             1  \n",
       "8229036             0               0             1  \n",
       "8229037             0               0             1  \n",
       "8229038             0               0             1  \n",
       "\n",
       "[8192732 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6b3a9",
   "metadata": {},
   "source": [
    "#### Feature Selection of top 30 models based on feature importance using RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eaefa063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing features and labels...\n",
      "Training a RandomForest model to find important features...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_estimators',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_estimators&nbsp;</td>\n",
       "            <td class=\"value\">50</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('criterion',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">criterion&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;gini&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_depth',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_depth&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_split',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_split&nbsp;</td>\n",
       "            <td class=\"value\">2</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_samples_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_samples_leaf&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_weight_fraction_leaf',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_weight_fraction_leaf&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_features',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_features&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;sqrt&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_leaf_nodes',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_leaf_nodes&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('min_impurity_decrease',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">min_impurity_decrease&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('bootstrap',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">bootstrap&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('oob_score',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">oob_score&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">-1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"user-set\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">42</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('ccp_alpha',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">ccp_alpha&nbsp;</td>\n",
       "            <td class=\"value\">0.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_samples',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_samples&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('monotonic_cst',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">monotonic_cst&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=50, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 3. Feature and Label Preparation ---\n",
    "print(\"Preparing features and labels...\")\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# --- 4. Train a Model for Feature Importance ---\n",
    "print(\"Training a RandomForest model to find important features...\")\n",
    "# We don't need to scale the data for a RandomForest's feature importance\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42, n_jobs=-1)\n",
    "model.fit(X, y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdc97c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting top 30 features...\n",
      "\n",
      "--- Top 30 Most Important Features ---\n",
      "['Init Fwd Win Byts', 'Fwd Seg Size Min', 'Dst Port', 'Fwd Header Len', 'Flow IAT Min', 'Flow Duration', 'Fwd IAT Max', 'Fwd IAT Min', 'Fwd Pkts/s', 'Flow Pkts/s', 'Fwd IAT Tot', 'Flow IAT Max', 'Fwd Pkt Len Mean', 'Fwd IAT Mean', 'Flow IAT Mean', 'Bwd Pkts/s', 'Pkt Len Mean', 'TotLen Fwd Pkts', 'Flow Byts/s', 'Init Bwd Win Byts', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Bwd Pkt Len Max', 'Pkt Len Var', 'Fwd Seg Size Avg', 'Tot Fwd Pkts', 'Bwd Header Len', 'Pkt Len Max', 'Subflow Fwd Byts', 'Subflow Fwd Pkts']\n",
      "\n",
      "Script finished successfully! ðŸŽ‰\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Get and Print Top Features ---\n",
    "print(\"Extracting top 30 features...\")\n",
    "\n",
    "# Create a dataframe of features and their importance scores\n",
    "importances = model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 30 feature names\n",
    "top_30_features = feature_importance_df.head(30)['feature'].tolist()\n",
    "\n",
    "print(\"\\n--- Top 30 Most Important Features ---\")\n",
    "# Print the list in a format that's easy to copy and paste\n",
    "print(repr(top_30_features))\n",
    "\n",
    "print(\"\\nScript finished successfully! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f0181a",
   "metadata": {},
   "source": [
    "#### Model Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9d10314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 30 available features for modeling.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Feature Selection & Label Prep ---\n",
    "# Using a representative list of top 30 features from this dataset.\n",
    "# Replace this with the exact list from your feature selection script if different.\n",
    "top_30_features = ['Init Fwd Win Byts', 'Fwd Seg Size Min', 'Dst Port', \n",
    "'Fwd Header Len', 'Flow IAT Min', 'Flow Duration', 'Fwd IAT Max', 'Fwd IAT Min', \n",
    "'Fwd Pkts/s', 'Flow Pkts/s', 'Fwd IAT Tot', 'Flow IAT Max', 'Fwd Pkt Len Mean', \n",
    "'Fwd IAT Mean', 'Flow IAT Mean', 'Bwd Pkts/s', 'Pkt Len Mean', 'TotLen Fwd Pkts', \n",
    "'Flow Byts/s', 'Init Bwd Win Byts', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', \n",
    "'Bwd Pkt Len Max', 'Pkt Len Var', 'Fwd Seg Size Avg', 'Tot Fwd Pkts', 'Bwd Header Len',\n",
    " 'Pkt Len Max', 'Subflow Fwd Byts', 'Subflow Fwd Pkts']\n",
    "\n",
    "\n",
    "# Ensure all selected features are present in the dataframe\n",
    "top_30_features = [f for f in top_30_features if f in df.columns]\n",
    "print(f\"Using {len(top_30_features)} available features for modeling.\")\n",
    "\n",
    "X = df[top_30_features]\n",
    "y = df['Label']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a59b85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Split and Scale Data ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7faecf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Initialize Models ---\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcb2617c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression ---\n",
      "--- Evaluating Logistic Regression ---\n",
      "\n",
      "Training Accuracy for Logistic Regression: 0.9439\n",
      "Testing Accuracy for Logistic Regression: 0.9438\n",
      "Classification Report for Logistic Regression (on Test Data):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.95      0.99      0.97   1215429\n",
      "                     Bot       0.82      0.50      0.62     57238\n",
      "        DDOS attack-HOIC       0.98      1.00      0.99    137203\n",
      "        DoS attacks-Hulk       0.99      1.00      0.99     92382\n",
      "DoS attacks-SlowHTTPTest       0.65      0.55      0.60     27978\n",
      "          FTP-BruteForce       0.71      0.78      0.74     38671\n",
      "           Infilteration       0.26      0.00      0.01     32128\n",
      "          SSH-Bruteforce       0.99      1.00      0.99     37518\n",
      "\n",
      "                accuracy                           0.94   1638547\n",
      "               macro avg       0.79      0.73      0.74   1638547\n",
      "            weighted avg       0.93      0.94      0.93   1638547\n",
      "\n",
      "\n",
      "--- Training Random Forest ---\n",
      "--- Evaluating Random Forest ---\n",
      "\n",
      "Training Accuracy for Random Forest: 0.9797\n",
      "Testing Accuracy for Random Forest: 0.9644\n",
      "Classification Report for Random Forest (on Test Data):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      0.99      0.98   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.77      0.51      0.62     27978\n",
      "          FTP-BruteForce       0.72      0.89      0.79     38671\n",
      "           Infilteration       0.21      0.10      0.13     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.96   1638547\n",
      "               macro avg       0.83      0.81      0.82   1638547\n",
      "            weighted avg       0.96      0.96      0.96   1638547\n",
      "\n",
      "\n",
      "--- Training XGBoost ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:41:12] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating XGBoost ---\n",
      "\n",
      "Training Accuracy for XGBoost: 0.9697\n",
      "Testing Accuracy for XGBoost: 0.9696\n",
      "Classification Report for XGBoost (on Test Data):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.98      1.00      0.99   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.77      0.51      0.62     27978\n",
      "          FTP-BruteForce       0.72      0.89      0.79     38671\n",
      "           Infilteration       0.55      0.03      0.06     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.97   1638547\n",
      "               macro avg       0.88      0.80      0.81   1638547\n",
      "            weighted avg       0.96      0.97      0.96   1638547\n",
      "\n",
      "\n",
      "--- Training LightGBM ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.455995 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 7387\n",
      "[LightGBM] [Info] Number of data points in the train set: 6554185, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -0.298712\n",
      "[LightGBM] [Info] Start training from score -3.354342\n",
      "[LightGBM] [Info] Start training from score -2.480109\n",
      "[LightGBM] [Info] Start training from score -2.875627\n",
      "[LightGBM] [Info] Start training from score -4.070146\n",
      "[LightGBM] [Info] Start training from score -3.746481\n",
      "[LightGBM] [Info] Start training from score -3.931845\n",
      "[LightGBM] [Info] Start training from score -3.776751\n",
      "--- Evaluating LightGBM ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy for LightGBM: 0.9716\n",
      "Testing Accuracy for LightGBM: 0.9715\n",
      "Classification Report for LightGBM (on Test Data):\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.97      1.00      0.99   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.91      0.53      0.66     27978\n",
      "          FTP-BruteForce       0.74      0.96      0.83     38671\n",
      "           Infilteration       0.59      0.03      0.05     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.97   1638547\n",
      "               macro avg       0.90      0.81      0.82   1638547\n",
      "            weighted avg       0.97      0.97      0.96   1638547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # --- 6. Train and Evaluate Each Model ---\n",
    "# for name, model in models.items():\n",
    "#     print(f\"\\n--- Training {name} ---\")\n",
    "#     model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "#     print(f\"--- Evaluating {name} ---\")\n",
    "#     y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     model_results[name] = accuracy\n",
    "    \n",
    "#     print(f\"\\nAccuracy for {name}: {accuracy:.4f}\")\n",
    "#     print(f\"Classification Report for {name}:\")\n",
    "#     print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# --- 6. Train and Evaluate Each Model ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"--- Evaluating {name} ---\")\n",
    "    \n",
    "    # 1. Calculate Training Accuracy (NEW)\n",
    "    y_train_pred = model.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # 2. Calculate Testing Accuracy (Original)\n",
    "    y_test_pred = model.predict(X_test_scaled)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    model_results[name] = test_accuracy # We still judge the model on its test accuracy\n",
    "    \n",
    "    # 3. Print both (NEW)\n",
    "    print(f\"\\nTraining Accuracy for {name}: {train_accuracy:.4f}\")\n",
    "    print(f\"Testing Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "    \n",
    "    print(f\"Classification Report for {name} (on Test Data):\")\n",
    "    print(classification_report(y_test, y_test_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53f612dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Comparison Summary ---\n",
      "                     Accuracy\n",
      "LightGBM             0.971511\n",
      "XGBoost              0.969581\n",
      "Random Forest        0.964376\n",
      "Logistic Regression  0.943799\n",
      "\n",
      "Best performing model is: LightGBM with an accuracy of 0.9715\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Compare Models and Save the Best One ---\n",
    "print(\"\\n--- Model Comparison Summary ---\")\n",
    "results_df = pd.DataFrame.from_dict(model_results, orient='index', columns=['Accuracy'])\n",
    "print(results_df.sort_values(by='Accuracy', ascending=False))\n",
    "\n",
    "best_model_name = max(model_results, key=model_results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest performing model is: {best_model_name} with an accuracy of {model_results[best_model_name]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2524384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the best model, scaler, and encoder...\n",
      "\n",
      "Script finished successfully! ðŸŽ‰\n",
      "The best model has been saved as 'best_detection_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving the best model, scaler, and encoder...\")\n",
    "joblib.dump(best_model, 'best_detection_model.joblib')\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder.joblib')\n",
    "\n",
    "print(\"\\nScript finished successfully! ðŸŽ‰\")\n",
    "print(\"The best model has been saved as 'best_detection_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0b3a4",
   "metadata": {},
   "source": [
    "#### Since classes were imbalanced, we try with SMOTE now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf41dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE  # Import SMOTE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57c1833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "filtered_df = pd.read_csv('../data/filtered_dataset.csv')\n",
    "print(\"Filtered dataset loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99ac5207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8229039 entries, 0 to 8229038\n",
      "Data columns (total 31 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Init Fwd Win Byts  int64  \n",
      " 1   Fwd Seg Size Min   int64  \n",
      " 2   Dst Port           int64  \n",
      " 3   Fwd Header Len     int64  \n",
      " 4   Flow IAT Min       float64\n",
      " 5   Flow Duration      int64  \n",
      " 6   Fwd IAT Max        float64\n",
      " 7   Fwd IAT Min        float64\n",
      " 8   Fwd Pkts/s         float64\n",
      " 9   Flow Pkts/s        float64\n",
      " 10  Fwd IAT Tot        float64\n",
      " 11  Flow IAT Max       float64\n",
      " 12  Fwd Pkt Len Mean   float64\n",
      " 13  Fwd IAT Mean       float64\n",
      " 14  Flow IAT Mean      float64\n",
      " 15  Bwd Pkts/s         float64\n",
      " 16  Pkt Len Mean       float64\n",
      " 17  TotLen Fwd Pkts    int64  \n",
      " 18  Flow Byts/s        float64\n",
      " 19  Init Bwd Win Byts  int64  \n",
      " 20  Bwd Pkt Len Mean   float64\n",
      " 21  Bwd Pkt Len Std    float64\n",
      " 22  Bwd Pkt Len Max    int64  \n",
      " 23  Pkt Len Var        float64\n",
      " 24  Fwd Seg Size Avg   float64\n",
      " 25  Tot Fwd Pkts       int64  \n",
      " 26  Bwd Header Len     int64  \n",
      " 27  Pkt Len Max        int64  \n",
      " 28  Subflow Fwd Byts   int64  \n",
      " 29  Subflow Fwd Pkts   int64  \n",
      " 30  Label              object \n",
      "dtypes: float64(17), int64(13), object(1)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8449098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Init Fwd Win Byts</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Fwd Header Len</th>\n",
       "      <th>Flow IAT Min</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Fwd IAT Max</th>\n",
       "      <th>Fwd IAT Min</th>\n",
       "      <th>Fwd Pkts/s</th>\n",
       "      <th>Flow Pkts/s</th>\n",
       "      <th>...</th>\n",
       "      <th>Bwd Pkt Len Std</th>\n",
       "      <th>Bwd Pkt Len Max</th>\n",
       "      <th>Pkt Len Var</th>\n",
       "      <th>Fwd Seg Size Avg</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Bwd Header Len</th>\n",
       "      <th>Pkt Len Max</th>\n",
       "      <th>Subflow Fwd Byts</th>\n",
       "      <th>Subflow Fwd Pkts</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56320761.0</td>\n",
       "      <td>112641719</td>\n",
       "      <td>56320958.0</td>\n",
       "      <td>56320761.0</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56320652.0</td>\n",
       "      <td>112641466</td>\n",
       "      <td>56320814.0</td>\n",
       "      <td>56320652.0</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>0.026633</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56319098.0</td>\n",
       "      <td>112638623</td>\n",
       "      <td>56319525.0</td>\n",
       "      <td>56319098.0</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>0.026634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65535</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>488</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6453966</td>\n",
       "      <td>673900.0</td>\n",
       "      <td>229740.0</td>\n",
       "      <td>2.324152</td>\n",
       "      <td>3.873587</td>\n",
       "      <td>...</td>\n",
       "      <td>371.677892</td>\n",
       "      <td>976</td>\n",
       "      <td>77192.153846</td>\n",
       "      <td>82.600000</td>\n",
       "      <td>15</td>\n",
       "      <td>328</td>\n",
       "      <td>976</td>\n",
       "      <td>1239</td>\n",
       "      <td>15</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5808</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>456</td>\n",
       "      <td>21.0</td>\n",
       "      <td>8804066</td>\n",
       "      <td>1928102.0</td>\n",
       "      <td>246924.0</td>\n",
       "      <td>1.590174</td>\n",
       "      <td>2.839597</td>\n",
       "      <td>...</td>\n",
       "      <td>362.249864</td>\n",
       "      <td>976</td>\n",
       "      <td>78267.353846</td>\n",
       "      <td>81.642857</td>\n",
       "      <td>14</td>\n",
       "      <td>360</td>\n",
       "      <td>976</td>\n",
       "      <td>1143</td>\n",
       "      <td>14</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Init Fwd Win Byts  Fwd Seg Size Min  Dst Port  Fwd Header Len  \\\n",
       "0                 -1                 0         0               0   \n",
       "1                 -1                 0         0               0   \n",
       "2                 -1                 0         0               0   \n",
       "3              65535                32        22             488   \n",
       "4               5808                32        22             456   \n",
       "\n",
       "   Flow IAT Min  Flow Duration  Fwd IAT Max  Fwd IAT Min  Fwd Pkts/s  \\\n",
       "0    56320761.0      112641719   56320958.0   56320761.0    0.026633   \n",
       "1    56320652.0      112641466   56320814.0   56320652.0    0.026633   \n",
       "2    56319098.0      112638623   56319525.0   56319098.0    0.026634   \n",
       "3          22.0        6453966     673900.0     229740.0    2.324152   \n",
       "4          21.0        8804066    1928102.0     246924.0    1.590174   \n",
       "\n",
       "   Flow Pkts/s  ...  Bwd Pkt Len Std  Bwd Pkt Len Max   Pkt Len Var  \\\n",
       "0     0.026633  ...         0.000000                0      0.000000   \n",
       "1     0.026633  ...         0.000000                0      0.000000   \n",
       "2     0.026634  ...         0.000000                0      0.000000   \n",
       "3     3.873587  ...       371.677892              976  77192.153846   \n",
       "4     2.839597  ...       362.249864              976  78267.353846   \n",
       "\n",
       "   Fwd Seg Size Avg  Tot Fwd Pkts  Bwd Header Len  Pkt Len Max  \\\n",
       "0          0.000000             3               0            0   \n",
       "1          0.000000             3               0            0   \n",
       "2          0.000000             3               0            0   \n",
       "3         82.600000            15             328          976   \n",
       "4         81.642857            14             360          976   \n",
       "\n",
       "   Subflow Fwd Byts  Subflow Fwd Pkts   Label  \n",
       "0                 0                 3  Benign  \n",
       "1                 0                 3  Benign  \n",
       "2                 0                 3  Benign  \n",
       "3              1239                15  Benign  \n",
       "4              1143                14  Benign  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ea57318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Label\n",
       "Benign                      6112151\n",
       "DDOS attack-HOIC             686012\n",
       "DoS attacks-Hulk             461912\n",
       "Bot                          286191\n",
       "FTP-BruteForce               193360\n",
       "SSH-Bruteforce               187589\n",
       "Infilteration                161934\n",
       "DoS attacks-SlowHTTPTest     139890\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343ce338",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.columns = filtered_df.columns.str.strip()\n",
    "\n",
    "# The dataset is known to have infinity and NaN values.\n",
    "# Replace infinite values with NaN\n",
    "filtered_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "# Drop rows with NaN values. For this dataset, this is a safe approach.\n",
    "filtered_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926222cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filtered_df.drop(columns=[\"Label\"])\n",
    "y = filtered_df['Label']\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6835be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced training script started...\n"
     ]
    }
   ],
   "source": [
    "print(\"Balanced training script started...\")\n",
    "# --- 4. Split and Scale Data ---\n",
    "# Split data BEFORE applying SMOTE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b87a045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler # Import the undersampler\n",
    "from imblearn.pipeline import Pipeline # Import the pipeline tool\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e92ffd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying a dynamic hybrid sampling strategy...\n",
      "The majority class has the numeric label: 0\n",
      "Second largest class has 548809 samples.\n",
      "Setting majority class target to 1646427 samples.\n",
      "Hybrid sampling applied. Training set is now smaller and balanced.\n",
      "New training set shape: (13171416, 30)\n",
      "Class distribution after resampling: Counter({np.int64(0): 1646427, np.int64(1): 1646427, np.int64(2): 1646427, np.int64(3): 1646427, np.int64(4): 1646427, np.int64(5): 1646427, np.int64(6): 1646427, np.int64(7): 1646427})\n"
     ]
    }
   ],
   "source": [
    "# # --- 5. Apply SMOTE to the Training Data ---\n",
    "# print(\"Applying SMOTE to the training data to handle imbalance...\")\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# print(\"SMOTE applied. Training set is now balanced.\")\n",
    "\n",
    "# --- 5. Define and Apply the Hybrid Sampling Strategy ---\n",
    "# print(\"Applying a hybrid undersampling and oversampling strategy...\")\n",
    "\n",
    "# # Define the undersampling strategy. \n",
    "# # This will reduce the 'Benign' class to have 500,000 samples. Adjust as needed.\n",
    "# # We first need to find the numerical label for 'Benign'\n",
    "# benign_label_numeric = label_encoder.transform(['Benign'])[0]\n",
    "# under_sampler = RandomUnderSampler(sampling_strategy={benign_label_numeric: 500000}, random_state=42)\n",
    "\n",
    "# # Define the SMOTE strategy. It will oversample all other classes to match the new majority.\n",
    "# over_sampler = SMOTE(random_state=42)\n",
    "\n",
    "# # Create a pipeline to apply the steps in sequence\n",
    "# pipeline = Pipeline(steps=[('u', under_sampler), ('o', over_sampler)])\n",
    "\n",
    "# # Apply the pipeline to the training data\n",
    "# X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# print(\"Hybrid sampling applied. Training set is now smaller and balanced.\")\n",
    "# print(f\"New training set shape: {X_train_resampled.shape}\")\n",
    "\n",
    "# --- 5. Define and Apply the Corrected Dynamic Hybrid Sampling Strategy ---\n",
    "print(\"Applying a dynamic hybrid sampling strategy...\")\n",
    "\n",
    "# Get the counts of each class in the training data\n",
    "class_counts = Counter(y_train)\n",
    "\n",
    "# --- THE FIX STARTS HERE ---\n",
    "# 1. Find the numeric label of the most common class (which is 'Benign')\n",
    "majority_class_label_numeric = class_counts.most_common(1)[0][0]\n",
    "print(f\"The majority class has the numeric label: {majority_class_label_numeric}\")\n",
    "\n",
    "# 2. Find the size of the second-largest class\n",
    "second_largest_class_size = sorted(class_counts.values(), reverse=True)[1]\n",
    "\n",
    "# 3. Set the undersampling target for the majority class\n",
    "majority_class_target_size = second_largest_class_size * 3\n",
    "print(f\"Second largest class has {second_largest_class_size} samples.\")\n",
    "print(f\"Setting majority class target to {majority_class_target_size} samples.\")\n",
    "# --- THE FIX ENDS HERE ---\n",
    "\n",
    "\n",
    "# Define the undersampling and oversampling strategies using the new variables\n",
    "under_sampler = RandomUnderSampler(sampling_strategy={majority_class_label_numeric: majority_class_target_size}, random_state=42)\n",
    "over_sampler = SMOTE(random_state=42)\n",
    "\n",
    "# Create and apply the pipeline\n",
    "pipeline = Pipeline(steps=[('u', under_sampler), ('o', over_sampler)])\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Hybrid sampling applied. Training set is now smaller and balanced.\")\n",
    "print(f\"New training set shape: {X_train_resampled.shape}\")\n",
    "print(f\"Class distribution after resampling: {Counter(y_train_resampled)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e44f811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Logistic Regression on Balanced Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating Logistic Regression on Original Test Data ---\n",
      "\n",
      "Accuracy for Logistic Regression: 0.5733\n",
      "Classification Report for Logistic Regression:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.99      0.45      0.62   1215429\n",
      "                     Bot       0.62      1.00      0.76     57238\n",
      "        DDOS attack-HOIC       0.97      1.00      0.99    137203\n",
      "        DoS attacks-Hulk       0.98      1.00      0.99     92382\n",
      "DoS attacks-SlowHTTPTest       0.64      0.55      0.59     27978\n",
      "          FTP-BruteForce       0.71      0.78      0.74     38671\n",
      "           Infilteration       0.04      0.77      0.07     32128\n",
      "          SSH-Bruteforce       0.97      1.00      0.99     37518\n",
      "\n",
      "                accuracy                           0.57   1638547\n",
      "               macro avg       0.74      0.82      0.72   1638547\n",
      "            weighted avg       0.94      0.57      0.67   1638547\n",
      "\n",
      "\n",
      "--- Training Random Forest on Balanced Data ---\n",
      "--- Evaluating Random Forest on Original Test Data ---\n",
      "\n",
      "Accuracy for Random Forest: 0.8087\n",
      "Classification Report for Random Forest:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.99      0.76      0.86   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.77      0.52      0.62     27978\n",
      "          FTP-BruteForce       0.72      0.88      0.79     38671\n",
      "           Infilteration       0.08      0.80      0.15     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.81   1638547\n",
      "               macro avg       0.82      0.87      0.80   1638547\n",
      "            weighted avg       0.97      0.81      0.87   1638547\n",
      "\n",
      "\n",
      "--- Training XGBoost on Balanced Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [01:43:48] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluating XGBoost on Original Test Data ---\n",
      "\n",
      "Accuracy for XGBoost: 0.8520\n",
      "Classification Report for XGBoost:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.99      0.82      0.90   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.76      0.52      0.62     27978\n",
      "          FTP-BruteForce       0.72      0.88      0.79     38671\n",
      "           Infilteration       0.09      0.70      0.17     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.85   1638547\n",
      "               macro avg       0.82      0.87      0.81   1638547\n",
      "            weighted avg       0.96      0.85      0.90   1638547\n",
      "\n",
      "\n",
      "--- Training LightGBM on Balanced Data ---\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.016258 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 7281\n",
      "[LightGBM] [Info] Number of data points in the train set: 13171416, number of used features: 30\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Info] Start training from score -2.079442\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "--- Evaluating LightGBM on Original Test Data ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy for LightGBM: 0.8721\n",
      "Classification Report for LightGBM:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "                  Benign       0.99      0.84      0.91   1215429\n",
      "                     Bot       1.00      1.00      1.00     57238\n",
      "        DDOS attack-HOIC       1.00      1.00      1.00    137203\n",
      "        DoS attacks-Hulk       1.00      1.00      1.00     92382\n",
      "DoS attacks-SlowHTTPTest       0.93      0.53      0.68     27978\n",
      "          FTP-BruteForce       0.74      0.97      0.84     38671\n",
      "           Infilteration       0.12      0.81      0.21     32128\n",
      "          SSH-Bruteforce       1.00      1.00      1.00     37518\n",
      "\n",
      "                accuracy                           0.87   1638547\n",
      "               macro avg       0.85      0.89      0.83   1638547\n",
      "            weighted avg       0.97      0.87      0.91   1638547\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Initialize Models ---\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "#     \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "#     \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "#     \"LightGBM\": LGBMClassifier(random_state=42)\n",
    "# }\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=20,         # <--- LIMITS tree depth to control size\n",
    "        min_samples_leaf=5,   # <--- Prevents overfitting to small groups\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    \"XGBoost\": XGBClassifier(\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        max_depth=15,         # <--- LIMITS tree depth\n",
    "        random_state=42\n",
    "    ),\n",
    "\n",
    "    \"LightGBM\": LGBMClassifier(\n",
    "        max_depth=15,         # <--- LIMITS tree depth\n",
    "        num_leaves=60,        # <--- Another way to control complexity\n",
    "        random_state=42\n",
    "    )\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "# --- 5. Train and Evaluate Each Model on Balanced Data ---\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Training {name} on Balanced Data ---\")\n",
    "    # Train on the NEW, resampled data\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    print(f\"--- Evaluating {name} on Original Test Data ---\")\n",
    "    # Evaluate on the ORIGINAL, unbalanced test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_results[name] = accuracy\n",
    "    \n",
    "    print(f\"\\nAccuracy for {name}: {accuracy:.4f}\")\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958e0504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Comparison Summary (Trained on Balanced Data) ---\n",
      "                     Accuracy\n",
      "LightGBM             0.872061\n",
      "XGBoost              0.852024\n",
      "Random Forest        0.808711\n",
      "Logistic Regression  0.573322\n",
      "\n",
      "Best performing model is: LightGBM with an accuracy of 0.8721\n",
      "Saving the best model, scaler, and encoder...\n",
      "\n",
      "Script finished successfully! ðŸŽ‰\n",
      "The best model has been saved as 'best_balanced_model.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Compare Models and Save the Best One ---\n",
    "print(\"\\n--- Model Comparison Summary (Trained on Balanced Data) ---\")\n",
    "results_df = pd.DataFrame.from_dict(model_results, orient='index', columns=['Accuracy'])\n",
    "print(results_df.sort_values(by='Accuracy', ascending=False))\n",
    "\n",
    "best_model_name = max(model_results, key=model_results.get)\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"\\nBest performing model is: {best_model_name} with an accuracy of {model_results[best_model_name]:.4f}\")\n",
    "\n",
    "print(\"Saving the best model, scaler, and encoder...\")\n",
    "joblib.dump(best_model, 'best_balanced_model_2nd.joblib')\n",
    "joblib.dump(scaler, 'scaler_balance_2nd.joblib')\n",
    "joblib.dump(label_encoder, 'label_encoder_balanced_2nd.joblib')\n",
    "\n",
    "print(\"\\nScript finished successfully! ðŸŽ‰\")\n",
    "print(\"The best model has been saved as 'best_balanced_model.joblib'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d5ada7",
   "metadata": {},
   "source": [
    "#### Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ffcd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score # Import f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd6ad68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load Your Filtered Data ---\n",
    "try:\n",
    "    df = pd.read_csv('../data/filtered_dataset.csv')\n",
    "    print(\"Filtered dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'filtered_dataset.csv' not found. Please create it first.\")\n",
    "    exit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97b51feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced infinite values with NaN.\n",
      "Dropped rows with NaN values. Data is now clean.\n"
     ]
    }
   ],
   "source": [
    "# --- THE FIX STARTS HERE ---\n",
    "# 1. Replace infinite values with NaN\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(\"Replaced infinite values with NaN.\")\n",
    "\n",
    "# 2. Drop all rows that now contain NaN\n",
    "df.dropna(inplace=True)\n",
    "print(\"Dropped rows with NaN values. Data is now clean.\")\n",
    "# --- THE FIX ENDS HERE ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f0e1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Using full benign set for Isolation Forest: 6077145 samples.\n",
      "Using smaller benign sample for LOF and One-Class SVM: 100000 samples.\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Prepare Data for Unsupervised Training ---\n",
    "print(\"Preparing data...\")\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Create a training set with ONLY 'Benign' (normal) data\n",
    "X_train_full_benign = X[y == 'Benign']\n",
    "\n",
    "# Create a smaller sample of benign data for the slower models\n",
    "X_train_sample_benign = X_train_full_benign.sample(n=100000, random_state=42)\n",
    "print(f\"Using full benign set for Isolation Forest: {len(X_train_full_benign)} samples.\")\n",
    "print(f\"Using smaller benign sample for LOF and One-Class SVM: {len(X_train_sample_benign)} samples.\")\n",
    "\n",
    "# The test set is the entire dataset\n",
    "X_test = X\n",
    "y_test = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "480e5ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Scale the Data ---\n",
    "# We fit the scaler ONLY on the full normal training data to learn the 'normal' distribution\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_full_benign)\n",
    "\n",
    "# Transform all datasets\n",
    "X_train_full_benign_scaled = scaler.transform(X_train_full_benign)\n",
    "X_train_sample_benign_scaled = scaler.transform(X_train_sample_benign)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc12d8",
   "metadata": {},
   "source": [
    "##### This is to check the percentage of the benign attacks in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c43ab590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Analysis ---\n",
      "Total Samples: 8192732\n",
      "Normal ('Benign') Samples: 6077145\n",
      "Anomaly (Attack) Samples: 2115587\n",
      "\n",
      "Exact Anomaly Percentage (Contamination): 0.2582\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Calculate the Percentage ---\n",
    "\n",
    "# Get the total number of rows\n",
    "total_samples = len(df)\n",
    "\n",
    "# Get the counts of each class in the 'Label' column\n",
    "label_counts = df['Label'].value_counts()\n",
    "\n",
    "# Get the number of 'Benign' (normal) samples\n",
    "benign_samples = label_counts['Benign']\n",
    "\n",
    "# The number of anomalies is everything that isn't 'Benign'\n",
    "anomaly_samples = total_samples - benign_samples\n",
    "\n",
    "# Calculate the contamination ratio (percentage of anomalies)\n",
    "contamination_ratio = anomaly_samples / total_samples\n",
    "\n",
    "# --- 3. Print the Results ---\n",
    "print(\"\\n--- Dataset Analysis ---\")\n",
    "print(f\"Total Samples: {total_samples}\")\n",
    "print(f\"Normal ('Benign') Samples: {benign_samples}\")\n",
    "print(f\"Anomaly (Attack) Samples: {anomaly_samples}\")\n",
    "print(f\"\\nExact Anomaly Percentage (Contamination): {contamination_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a57f30",
   "metadata": {},
   "source": [
    "##### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4fdb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # --- 4. Initialize Models OLD ---\n",
    "# # models = {\n",
    "# #     \"Isolation Forest\": {\n",
    "# #         \"model\": IsolationForest(n_estimators=100, contamination='auto', random_state=42, n_jobs=-1),\n",
    "# #         \"train_data\": X_train_full_benign_scaled\n",
    "# #     },\n",
    "# #     \"Local Outlier Factor\": {\n",
    "# #         # novelty=True is essential for predicting on new data\n",
    "# #         \"model\": LocalOutlierFactor(n_neighbors=20, contamination='auto', novelty=True, n_jobs=-1),\n",
    "# #         \"train_data\": X_train_sample_benign_scaled\n",
    "# #     },\n",
    "# #     \"One-Class SVM\": {\n",
    "# #         \"model\": OneClassSVM(nu=0.01, kernel=\"rbf\", gamma='auto'),\n",
    "# #         \"train_data\": X_train_sample_benign_scaled\n",
    "# #     }\n",
    "# # }\n",
    "\n",
    "# # The actual anomaly ratio in your data is ~0.25 (2,115,587 / 8,192,732)\n",
    "# known_contamination = 0.25 \n",
    "\n",
    "# models = {\n",
    "#     \"Isolation Forest\": {\n",
    "#         # FIX: Change contamination from 'auto' to the known ratio to increase sensitivity.\n",
    "#         \"model\": IsolationForest(\n",
    "#             n_estimators=100, \n",
    "#             contamination=known_contamination, #<-- THE FIX\n",
    "#             random_state=42, \n",
    "#             n_jobs=-1\n",
    "#         ),\n",
    "#         \"train_data\": X_train_full_benign_scaled\n",
    "#     },\n",
    "#     \"Local Outlier Factor\": {\n",
    "#         # TUNE: Adjust n_neighbors to fine-tune the precision/recall balance.\n",
    "#         # Start with a larger value like 30 to potentially reduce false positives.\n",
    "#         \"model\": LocalOutlierFactor(\n",
    "#             n_neighbors=30, #<-- TUNE THIS\n",
    "#             contamination=known_contamination, \n",
    "#             novelty=True, \n",
    "#             n_jobs=-1\n",
    "#         ),\n",
    "#         \"train_data\": X_train_sample_benign_scaled\n",
    "#     },\n",
    "#     \"One-Class SVM\": {\n",
    "#         # FIX: Increase 'nu' to make the model less conservative and catch more anomalies.\n",
    "#         \"model\": OneClassSVM(\n",
    "#             nu=known_contamination, #<-- THE FIX\n",
    "#             kernel=\"rbf\", \n",
    "#             gamma='auto'\n",
    "#         ),\n",
    "#         \"train_data\": X_train_sample_benign_scaled\n",
    "#     }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88625579",
   "metadata": {},
   "source": [
    "New Code for better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bf11ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11629859",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known contamination ratio in dataset\n",
    "known_contamination = 0.2582  \n",
    "\n",
    "# =======================\n",
    "# Isolation Forest (IF)\n",
    "# =======================\n",
    "isolation_forest_params = {\n",
    "    \"n_estimators\": 300,      # more trees for stability\n",
    "    \"max_samples\": 0.5,       # sample half the data per tree\n",
    "    \"contamination\": known_contamination,\n",
    "    \"bootstrap\": True,        # improves variance\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Local Outlier Factor (LOF)\n",
    "# =======================\n",
    "lof_params = {\n",
    "    \"n_neighbors\": 50,         # <-- try 20, 30, 50, 100 in sweeps\n",
    "    \"contamination\": known_contamination,\n",
    "    \"novelty\": True,           # needed for predicting on new data\n",
    "    \"n_jobs\": -1\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# One-Class SVM (OCSVM)\n",
    "# =======================\n",
    "# Subsample benign training data to keep memory manageable\n",
    "sample_size = 100_000\n",
    "if len(X_train_sample_benign_scaled) > sample_size:\n",
    "    X_ocsvm_train = X_train_sample_benign_scaled.sample(n=sample_size, random_state=42)\n",
    "else:\n",
    "    X_ocsvm_train = X_train_sample_benign_scaled\n",
    "\n",
    "# Option A: Simple linear kernel\n",
    "ocsvm_linear = OneClassSVM(\n",
    "    nu=known_contamination,\n",
    "    kernel=\"linear\"\n",
    ")\n",
    "\n",
    "# Option B: RBF kernel approximation + linear OCSVM\n",
    "rbf_feature = RBFSampler(gamma=1, n_components=300, random_state=42)\n",
    "ocsvm_rbf_approx = make_pipeline(\n",
    "    rbf_feature,\n",
    "    OneClassSVM(nu=known_contamination, kernel=\"linear\")\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Collect models\n",
    "# =======================\n",
    "models = {\n",
    "    \"Isolation Forest\": {\n",
    "        \"model\": IsolationForest(**isolation_forest_params),\n",
    "        \"train_data\": X_train_full_benign_scaled\n",
    "    },\n",
    "    \"Local Outlier Factor\": {\n",
    "        \"model\": LocalOutlierFactor(**lof_params),\n",
    "        \"train_data\": X_train_sample_benign_scaled\n",
    "    },\n",
    "    \"One-Class SVM (Linear)\": {\n",
    "        \"model\": ocsvm_linear,\n",
    "        \"train_data\": X_ocsvm_train\n",
    "    },\n",
    "    \"One-Class SVM (RBF Approx)\": {\n",
    "        \"model\": ocsvm_rbf_approx,\n",
    "        \"train_data\": X_ocsvm_train\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eaaea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Isolation Forest ---\n",
      "--- Evaluating Isolation Forest ---\n",
      "*** New best model found: Isolation Forest with Anomaly F1-Score: 0.3390 ***\n",
      "\n",
      "Anomaly Detection Report for Isolation Forest:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Anomaly (-1)       0.32      0.36      0.34   2115587\n",
      "  Normal (1)       0.77      0.74      0.75   6077145\n",
      "\n",
      "    accuracy                           0.64   8192732\n",
      "   macro avg       0.55      0.55      0.55   8192732\n",
      "weighted avg       0.65      0.64      0.65   8192732\n",
      "\n",
      "Confusion Matrix for Isolation Forest:\n",
      " [[ 751936 1363651]\n",
      " [1569119 4508026]]\n",
      "\n",
      "--- Training Local Outlier Factor ---\n",
      "--- Evaluating Local Outlier Factor ---\n",
      "*** New best model found: Local Outlier Factor with Anomaly F1-Score: 0.6701 ***\n",
      "\n",
      "Anomaly Detection Report for Local Outlier Factor:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Anomaly (-1)       0.54      0.88      0.67   2115587\n",
      "  Normal (1)       0.95      0.74      0.83   6077145\n",
      "\n",
      "    accuracy                           0.78   8192732\n",
      "   macro avg       0.74      0.81      0.75   8192732\n",
      "weighted avg       0.84      0.78      0.79   8192732\n",
      "\n",
      "Confusion Matrix for Local Outlier Factor:\n",
      " [[1870616  244971]\n",
      " [1597033 4480112]]\n",
      "\n",
      "--- Training One-Class SVM (Linear) ---\n"
     ]
    }
   ],
   "source": [
    "# # --- 5. Train and Evaluate Each Model ---\n",
    "# # Convert true labels to the 1/-1 format for evaluation\n",
    "# y_test_mapped = y_test.apply(lambda x: 1 if x == 'Benign' else -1)\n",
    "\n",
    "# for name, model_info in models.items():\n",
    "#     print(f\"\\n--- Training {name} ---\")\n",
    "#     model_instance = model_info[\"model\"]\n",
    "#     train_data = model_info[\"train_data\"]\n",
    "    \n",
    "#     model_instance.fit(train_data)\n",
    "    \n",
    "#     print(f\"--- Evaluating {name} ---\")\n",
    "#     y_pred = model_instance.predict(X_test_scaled)\n",
    "    \n",
    "#     print(f\"\\nAnomaly Detection Report for {name}:\")\n",
    "#     print(classification_report(y_test_mapped, y_pred, target_names=['Anomaly (-1)', 'Normal (1)']))\n",
    "#     print(f\"Confusion Matrix for {name}:\\n\", confusion_matrix(y_test_mapped, y_pred))\n",
    "\n",
    "# print(\"\\nScript finished successfully! ðŸŽ‰\")\n",
    "\n",
    "# --- 5. Train, Evaluate, and Find the Best Model ---\n",
    "y_test_mapped = y_test.apply(lambda x: 1 if x == 'Benign' else -1)\n",
    "\n",
    "# Variables to track the best model\n",
    "best_f1_score = -1\n",
    "best_model_name = \"\"\n",
    "best_model_to_save = None\n",
    "\n",
    "for name, model_info in models.items():\n",
    "    print(f\"\\n--- Training {name} ---\")\n",
    "    model_instance = model_info[\"model\"]\n",
    "    train_data = model_info[\"train_data\"]\n",
    "    \n",
    "    model_instance.fit(train_data)\n",
    "    \n",
    "    print(f\"--- Evaluating {name} ---\")\n",
    "    y_pred = model_instance.predict(X_test_scaled)\n",
    "    \n",
    "    # --- NEW LOGIC: TRACK THE BEST MODEL ---\n",
    "    # Calculate F1-score specifically for the anomaly class (-1)\n",
    "    current_f1 = f1_score(y_test_mapped, y_pred, pos_label=-1)\n",
    "    \n",
    "    if current_f1 > best_f1_score:\n",
    "        best_f1_score = current_f1\n",
    "        best_model_name = name\n",
    "        best_model_to_save = model_instance\n",
    "        print(f\"*** New best model found: {name} with Anomaly F1-Score: {current_f1:.4f} ***\")\n",
    "    # --- END OF NEW LOGIC ---\n",
    "\n",
    "    print(f\"\\nAnomaly Detection Report for {name}:\")\n",
    "    print(classification_report(y_test_mapped, y_pred, target_names=['Anomaly (-1)', 'Normal (1)']))\n",
    "    print(f\"Confusion Matrix for {name}:\\n\", confusion_matrix(y_test_mapped, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0450a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Save the Final Best Model ---\n",
    "print(\"\\n--- Evaluation Complete ---\")\n",
    "if best_model_to_save:\n",
    "    print(f\"The best model is '{best_model_name}' with an Anomaly F1-Score of {best_f1_score:.4f}\")\n",
    "    \n",
    "    print(\"\\nSaving the best unsupervised model and its scaler...\")\n",
    "    joblib.dump(best_model_to_save, 'best_unsupervised_model.joblib')\n",
    "    joblib.dump(scaler, 'unsupervised_scaler.joblib')\n",
    "    print(\"Files saved successfully!\")\n",
    "else:\n",
    "    print(\"No model was saved as no evaluation was successful.\")\n",
    "\n",
    "print(\"\\nScript finished successfully! ðŸŽ‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe16301",
   "metadata": {},
   "source": [
    "#### Selecting features for Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86a0fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "317fce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/combined_realistic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaf3b36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numerical features: 78\n"
     ]
    }
   ],
   "source": [
    "# Remove the label column for unsupervised feature selection\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Total numerical features: {len(numerical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a078a72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunks = pd.read_csv(\"../data/combined_realistic.csv\", chunksize=500_000)\n",
    "clean_chunks = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    chunk = chunk.replace([np.inf, -np.inf], np.nan)\n",
    "    chunk = chunk.dropna()\n",
    "    clean_chunks.append(chunk)\n",
    "\n",
    "df = pd.concat(clean_chunks, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9e957ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "METHOD 1: Variance-Based Feature Selection\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 4.78 GiB for an array with shape (8229039, 78) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimpute\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SimpleImputer\n\u001b[32m     16\u001b[39m imputer = SimpleImputer(strategy=\u001b[33m'\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m X_imputed = \u001b[43mimputer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m X_imputed = pd.DataFrame(X_imputed, columns=numerical_features)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Standardize the features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\base.py:894\u001b[39m, in \u001b[36mTransformerMixin.fit_transform\u001b[39m\u001b[34m(self, X, y, **fit_params)\u001b[39m\n\u001b[32m    879\u001b[39m         warnings.warn(\n\u001b[32m    880\u001b[39m             (\n\u001b[32m    881\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) has a `transform`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    889\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m    890\u001b[39m         )\n\u001b[32m    892\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    893\u001b[39m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m.transform(X)\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    896\u001b[39m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[32m    897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:469\u001b[39m, in \u001b[36mSimpleImputer.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28mself\u001b[39m.statistics_ = \u001b[38;5;28mself\u001b[39m._sparse_fit(\n\u001b[32m    466\u001b[39m         X, \u001b[38;5;28mself\u001b[39m.strategy, \u001b[38;5;28mself\u001b[39m.missing_values, fill_value\n\u001b[32m    467\u001b[39m     )\n\u001b[32m    468\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m     \u001b[38;5;28mself\u001b[39m.statistics_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dense_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmissing_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\sklearn\\impute\\_base.py:551\u001b[39m, in \u001b[36mSimpleImputer._dense_fit\u001b[39m\u001b[34m(self, X, strategy, missing_values, fill_value)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# Median\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m strategy == \u001b[33m\"\u001b[39m\u001b[33mmedian\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     median_masked = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasked_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m     \u001b[38;5;66;03m# Avoid the warning \"Warning: converting a masked element to nan.\"\u001b[39;00m\n\u001b[32m    553\u001b[39m     median = np.ma.getdata(median_masked)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\extras.py:810\u001b[39m, in \u001b[36mmedian\u001b[39m\u001b[34m(a, axis, out, overwrite_input, keepdims)\u001b[39m\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    808\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m m\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_median\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    811\u001b[39m \u001b[43m                \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\lib\\_function_base_impl.py:3914\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3911\u001b[39m     index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3912\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3914\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3916\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\extras.py:829\u001b[39m, in \u001b[36m_median\u001b[39m\u001b[34m(a, axis, out, overwrite_input)\u001b[39m\n\u001b[32m    827\u001b[39m         asorted = a\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     asorted = \u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    832\u001b[39m     axis = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:7269\u001b[39m, in \u001b[36msort\u001b[39m\u001b[34m(a, axis, kind, order, endwith, fill_value, stable)\u001b[39m\n\u001b[32m   7266\u001b[39m     axis = \u001b[32m0\u001b[39m\n\u001b[32m   7268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(a, MaskedArray):\n\u001b[32m-> \u001b[39m\u001b[32m7269\u001b[39m     \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendwith\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendwith\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   7270\u001b[39m \u001b[43m           \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7271\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   7272\u001b[39m     a.sort(axis=axis, kind=kind, order=order, stable=stable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:5871\u001b[39m, in \u001b[36mMaskedArray.sort\u001b[39m\u001b[34m(self, axis, kind, order, endwith, fill_value, stable)\u001b[39m\n\u001b[32m   5868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m masked:\n\u001b[32m   5869\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5871\u001b[39m sidx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5872\u001b[39m \u001b[43m                    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendwith\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendwith\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5874\u001b[39m \u001b[38;5;28mself\u001b[39m[...] = np.take_along_axis(\u001b[38;5;28mself\u001b[39m, sidx, axis=axis)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:5694\u001b[39m, in \u001b[36mMaskedArray.argsort\u001b[39m\u001b[34m(self, axis, kind, order, endwith, fill_value, stable)\u001b[39m\n\u001b[32m   5691\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   5692\u001b[39m         fill_value = maximum_fill_value(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m5694\u001b[39m filled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5695\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m filled.argsort(axis=axis, kind=kind, order=order)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\VII\\ANT\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:3931\u001b[39m, in \u001b[36mMaskedArray.filled\u001b[39m\u001b[34m(self, fill_value)\u001b[39m\n\u001b[32m   3929\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data\n\u001b[32m   3930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3931\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mK\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3932\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3933\u001b[39m         np.copyto(result, fill_value, where=m)\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 4.78 GiB for an array with shape (8229039, 78) and data type float64"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# METHOD 1: Variance-Based Selection\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 1: Variance-Based Feature Selection\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate variance for each feature\n",
    "X = df[numerical_features].copy()\n",
    "\n",
    "# Handle infinite values\n",
    "X = X.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute missing values with median\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_imputed = pd.DataFrame(X_imputed, columns=numerical_features)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=numerical_features)\n",
    "\n",
    "# Calculate variance\n",
    "variances = X_scaled.var().sort_values(ascending=False)\n",
    "print(\"\\nTop 25 features by variance:\")\n",
    "variance_top_25 = variances.head(25).index.tolist()\n",
    "for i, (feat, var) in enumerate(variances.head(25).items(), 1):\n",
    "    print(f\"{i:2d}. {feat:30s} - Variance: {var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6797c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 2: Correlation-Based Removal\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 2: Low Correlation Feature Selection\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = X_imputed.corr().abs()\n",
    "\n",
    "# Remove highly correlated features (keeping diverse features)\n",
    "upper_triangle = corr_matrix.where(\n",
    "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper_triangle.columns \n",
    "           if any(upper_triangle[column] > 0.95)]\n",
    "\n",
    "print(f\"\\nHighly correlated features to remove: {len(to_drop)}\")\n",
    "print(to_drop[:10] if len(to_drop) > 10 else to_drop)\n",
    "\n",
    "# Keep features with low correlation\n",
    "reduced_features = [f for f in numerical_features if f not in to_drop]\n",
    "print(f\"\\nFeatures after correlation removal: {len(reduced_features)}\")\n",
    "\n",
    "# Select top 25 by variance from reduced set\n",
    "X_reduced = X_scaled[reduced_features]\n",
    "reduced_variances = X_reduced.var().sort_values(ascending=False)\n",
    "correlation_top_25 = reduced_variances.head(25).index.tolist()\n",
    "\n",
    "print(\"\\nTop 25 features after correlation filtering:\")\n",
    "for i, feat in enumerate(correlation_top_25, 1):\n",
    "    print(f\"{i:2d}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1959a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 3: PCA-Based Feature Importance\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 3: PCA-Based Feature Importance\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=25)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Calculate feature importance based on PCA loadings\n",
    "# Sum of absolute loadings across first 25 components\n",
    "feature_importance_pca = np.abs(pca.components_).sum(axis=0)\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': numerical_features,\n",
    "    'importance': feature_importance_pca\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "pca_top_25 = feature_importance_df.head(25)['feature'].tolist()\n",
    "\n",
    "print(f\"\\nExplained variance ratio (first 25 components): {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "print(\"\\nTop 25 features by PCA importance:\")\n",
    "for i, row in feature_importance_df.head(25).iterrows():\n",
    "    print(f\"{list(feature_importance_df.index).index(i)+1:2d}. {row['feature']:30s} - Importance: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 4: Isolation Forest-Based Selection\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 4: Isolation Forest Feature Importance\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample data for faster computation (if dataset is large)\n",
    "sample_size = min(50000, len(X_scaled))\n",
    "X_sample = X_scaled.sample(n=sample_size, random_state=42)\n",
    "\n",
    "# Train Isolation Forest\n",
    "iso_forest = IsolationForest(n_estimators=100, random_state=42, contamination=0.1)\n",
    "iso_forest.fit(X_sample)\n",
    "\n",
    "# Calculate feature importance using permutation importance concept\n",
    "feature_importance_if = []\n",
    "baseline_score = iso_forest.score_samples(X_sample).mean()\n",
    "\n",
    "for feature in numerical_features:\n",
    "    X_permuted = X_sample.copy()\n",
    "    X_permuted[feature] = np.random.permutation(X_permuted[feature])\n",
    "    permuted_score = iso_forest.score_samples(X_permuted).mean()\n",
    "    importance = abs(baseline_score - permuted_score)\n",
    "    feature_importance_if.append(importance)\n",
    "\n",
    "if_importance_df = pd.DataFrame({\n",
    "    'feature': numerical_features,\n",
    "    'importance': feature_importance_if\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "if_top_25 = if_importance_df.head(25)['feature'].tolist()\n",
    "\n",
    "print(\"\\nTop 25 features by Isolation Forest importance:\")\n",
    "for i, row in if_importance_df.head(25).iterrows():\n",
    "    print(f\"{list(if_importance_df.index).index(i)+1:2d}. {row['feature']:30s} - Importance: {row['importance']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa01eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# METHOD 5: Ensemble Voting (RECOMMENDED)\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"METHOD 5: Ensemble Voting (RECOMMENDED)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Count votes from all methods\n",
    "all_features = variance_top_25 + correlation_top_25 + pca_top_25 + if_top_25\n",
    "feature_votes = pd.Series(all_features).value_counts()\n",
    "\n",
    "print(\"\\nFeature selection votes (max 4 votes):\")\n",
    "final_top_25 = feature_votes.head(25).index.tolist()\n",
    "\n",
    "for i, (feat, votes) in enumerate(feature_votes.head(25).items(), 1):\n",
    "    methods = []\n",
    "    if feat in variance_top_25: methods.append(\"VAR\")\n",
    "    if feat in correlation_top_25: methods.append(\"COR\")\n",
    "    if feat in pca_top_25: methods.append(\"PCA\")\n",
    "    if feat in if_top_25: methods.append(\"IF\")\n",
    "    print(f\"{i:2d}. {feat:30s} - Votes: {votes}/4 [{', '.join(methods)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a0df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Save Results\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create results dictionary\n",
    "results = {\n",
    "    'variance_based': variance_top_25,\n",
    "    'correlation_filtered': correlation_top_25,\n",
    "    'pca_based': pca_top_25,\n",
    "    'isolation_forest': if_top_25,\n",
    "    'ensemble_voting': final_top_25\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "with open('top_25_features.txt', 'w') as f:\n",
    "    for method, features in results.items():\n",
    "        f.write(f\"\\n{'='*60}\\n\")\n",
    "        f.write(f\"{method.upper().replace('_', ' ')}\\n\")\n",
    "        f.write(f\"{'='*60}\\n\")\n",
    "        for i, feat in enumerate(features, 1):\n",
    "            f.write(f\"{i:2d}. {feat}\\n\")\n",
    "\n",
    "print(\"\\nâœ“ Results saved to 'top_25_features.txt'\")\n",
    "\n",
    "print(\"\\nRECOMMENDED FINAL SET (Ensemble Voting):\")\n",
    "print(\"=\"*50)\n",
    "print(final_top_25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909835cf",
   "metadata": {},
   "source": [
    "#### After gz file Unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2bcaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#  Load cleaned dataset\n",
    "df = pd.read_csv(\"../data/combined_realistic_clean.csv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f9b92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Select numeric features only\n",
    "X = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "\n",
    "#  Remove low-variance features\n",
    "var_thresh = VarianceThreshold(threshold=0.01)\n",
    "X_var = var_thresh.fit_transform(X)\n",
    "X_var_cols = X.columns[var_thresh.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b0a8a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 25 features for Isolation Forest / LOF:\n",
      "Index(['Init Fwd Win Byts', 'Down/Up Ratio', 'URG Flag Cnt', 'Dst Port',\n",
      "       'Fwd Pkt Len Min', 'Fwd PSH Flags', 'Init Bwd Win Byts', 'Pkt Len Var',\n",
      "       'RST Flag Cnt', 'Fwd Seg Size Min', 'Tot Bwd Pkts', 'Fwd Pkt Len Max',\n",
      "       'Bwd Pkt Len Mean', 'Flow Byts/s', 'Bwd IAT Std', 'Fwd Header Len',\n",
      "       'Bwd IAT Min', 'TotLen Fwd Pkts', 'Active Mean', 'ACK Flag Cnt',\n",
      "       'Bwd Pkt Len Min', 'Active Std', 'Idle Min', 'Flow Pkts/s',\n",
      "       'PSH Flag Cnt'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#  Remove highly correlated features\n",
    "X_var_df = pd.DataFrame(X_var, columns=X_var_cols)\n",
    "corr_matrix = X_var_df.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "X_uncorr = X_var_df.drop(columns=to_drop)\n",
    "\n",
    "#  Scale features (important for IF/LOF)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_uncorr)\n",
    "\n",
    "#  PCA for feature importance estimation\n",
    "pca = PCA(n_components=min(25, X_scaled.shape[1]))\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "importance = np.abs(pca.components_).sum(axis=0)\n",
    "top_idx = importance.argsort()[-25:][::-1]\n",
    "top_features = X_uncorr.columns[top_idx]\n",
    "\n",
    "print(\"Top 25 features for Isolation Forest / LOF:\")\n",
    "print(top_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae09e9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../data/combined_realistic_top30.csv.gz with 4-decimal precision\n"
     ]
    }
   ],
   "source": [
    "selected_features = [\n",
    "    'Init Fwd Win Byts', 'Down/Up Ratio', 'URG Flag Cnt', 'Dst Port',\n",
    "    'Fwd Pkt Len Min', 'Fwd PSH Flags', 'Init Bwd Win Byts', 'Pkt Len Var',\n",
    "    'RST Flag Cnt', 'Fwd Seg Size Min', 'Tot Bwd Pkts', 'Fwd Pkt Len Max',\n",
    "    'Bwd Pkt Len Mean', 'Flow Byts/s', 'Bwd IAT Std', 'Fwd Header Len',\n",
    "    'Bwd IAT Min', 'TotLen Fwd Pkts', 'Active Mean', 'ACK Flag Cnt',\n",
    "    'Bwd Pkt Len Min', 'Active Std', 'Idle Min', 'Flow Pkts/s',\n",
    "    'PSH Flag Cnt', 'Flow Duration', 'Fwd IAT Mean', 'Bwd IAT Mean',\n",
    "    'Bwd Pkt Len Std', 'Tot Fwd Pkts'\n",
    "]\n",
    "\n",
    "df_selected = df.loc[:, selected_features + ['Label']].copy()\n",
    "\n",
    "df_selected.to_csv(\n",
    "    \"../data/combined_realistic_top30.csv.gz\",\n",
    "    index=False,\n",
    "    compression=\"gzip\",\n",
    "    float_format=\"%.4f\"\n",
    ")\n",
    "\n",
    "print(\"Saved: ../data/combined_realistic_top30.csv.gz with 4-decimal precision\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022ca1eb",
   "metadata": {},
   "source": [
    "*TRAINING THE UNSUP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8ac7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6fad802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 1ï¸âƒ£ Load the dataset\n",
    "df = pd.read_csv(\"../data/combined_realistic_top30.csv.gz\")\n",
    "\n",
    "# 2ï¸âƒ£ Split features and labels (if available)\n",
    "X = df.drop(columns=['Label'], errors='ignore')\n",
    "y = df['Label'] if 'Label' in df.columns else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad503fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8192732 entries, 0 to 8192731\n",
      "Data columns (total 31 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Init Fwd Win Byts  int64  \n",
      " 1   Down/Up Ratio      int64  \n",
      " 2   URG Flag Cnt       int64  \n",
      " 3   Dst Port           int64  \n",
      " 4   Fwd Pkt Len Min    int64  \n",
      " 5   Fwd PSH Flags      int64  \n",
      " 6   Init Bwd Win Byts  int64  \n",
      " 7   Pkt Len Var        float64\n",
      " 8   RST Flag Cnt       int64  \n",
      " 9   Fwd Seg Size Min   int64  \n",
      " 10  Tot Bwd Pkts       int64  \n",
      " 11  Fwd Pkt Len Max    int64  \n",
      " 12  Bwd Pkt Len Mean   float64\n",
      " 13  Flow Byts/s        float64\n",
      " 14  Bwd IAT Std        float64\n",
      " 15  Fwd Header Len     int64  \n",
      " 16  Bwd IAT Min        float64\n",
      " 17  TotLen Fwd Pkts    int64  \n",
      " 18  Active Mean        float64\n",
      " 19  ACK Flag Cnt       int64  \n",
      " 20  Bwd Pkt Len Min    int64  \n",
      " 21  Active Std         float64\n",
      " 22  Idle Min           float64\n",
      " 23  Flow Pkts/s        float64\n",
      " 24  PSH Flag Cnt       int64  \n",
      " 25  Flow Duration      int64  \n",
      " 26  Fwd IAT Mean       float64\n",
      " 27  Bwd IAT Mean       float64\n",
      " 28  Bwd Pkt Len Std    float64\n",
      " 29  Tot Fwd Pkts       int64  \n",
      " 30  Label              object \n",
      "dtypes: float64(12), int64(18), object(1)\n",
      "memory usage: 1.9+ GB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d1ba6eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Benign', 'FTP-BruteForce', 'SSH-Bruteforce',\n",
       "       'DoS attacks-SlowHTTPTest', 'DoS attacks-Hulk', 'DDOS attack-HOIC',\n",
       "       'Infilteration', 'Bot'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e69c785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Init Fwd Win Byts</th>\n",
       "      <th>Down/Up Ratio</th>\n",
       "      <th>URG Flag Cnt</th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Fwd Pkt Len Min</th>\n",
       "      <th>Fwd PSH Flags</th>\n",
       "      <th>Init Bwd Win Byts</th>\n",
       "      <th>Pkt Len Var</th>\n",
       "      <th>RST Flag Cnt</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>...</th>\n",
       "      <th>Bwd Pkt Len Min</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Flow Pkts/s</th>\n",
       "      <th>PSH Flag Cnt</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Fwd IAT Mean</th>\n",
       "      <th>Bwd IAT Mean</th>\n",
       "      <th>Bwd Pkt Len Std</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "      <td>8.192732e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.236113e+04</td>\n",
       "      <td>4.586415e-01</td>\n",
       "      <td>4.536545e-02</td>\n",
       "      <td>1.079241e+04</td>\n",
       "      <td>9.020077e+00</td>\n",
       "      <td>2.921431e-02</td>\n",
       "      <td>7.876900e+03</td>\n",
       "      <td>4.045124e+04</td>\n",
       "      <td>1.654723e-01</td>\n",
       "      <td>1.985582e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.084202e+01</td>\n",
       "      <td>4.098663e+04</td>\n",
       "      <td>4.348001e+06</td>\n",
       "      <td>7.123932e+04</td>\n",
       "      <td>3.934636e-01</td>\n",
       "      <td>1.009598e+07</td>\n",
       "      <td>3.772823e+06</td>\n",
       "      <td>5.547543e+05</td>\n",
       "      <td>1.210698e+02</td>\n",
       "      <td>5.328744e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.979651e+04</td>\n",
       "      <td>7.159337e-01</td>\n",
       "      <td>2.081044e-01</td>\n",
       "      <td>2.014743e+04</td>\n",
       "      <td>2.407408e+01</td>\n",
       "      <td>1.684068e-01</td>\n",
       "      <td>1.924684e+04</td>\n",
       "      <td>2.063026e+05</td>\n",
       "      <td>3.716063e-01</td>\n",
       "      <td>8.445891e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.691458e+01</td>\n",
       "      <td>8.598273e+05</td>\n",
       "      <td>8.831305e+07</td>\n",
       "      <td>2.995297e+05</td>\n",
       "      <td>4.885182e-01</td>\n",
       "      <td>6.946322e+08</td>\n",
       "      <td>3.136042e+08</td>\n",
       "      <td>2.828583e+06</td>\n",
       "      <td>1.894860e+02</td>\n",
       "      <td>7.954374e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e-02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-9.190110e+11</td>\n",
       "      <td>-8.282200e+11</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.800000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.300000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.720000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.280000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.052000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>8.000000e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>7.363300e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.555300e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.313600e+04</td>\n",
       "      <td>4.028500e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>8.192000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.389000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.300000e+02</td>\n",
       "      <td>8.452794e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.511810e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.567321e+06</td>\n",
       "      <td>2.945797e+05</td>\n",
       "      <td>2.623100e+04</td>\n",
       "      <td>2.090000e+02</td>\n",
       "      <td>5.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>2.370000e+02</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>1.460000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.553500e+04</td>\n",
       "      <td>5.190000e+08</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.600000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.460000e+03</td>\n",
       "      <td>7.495336e+07</td>\n",
       "      <td>2.399340e+11</td>\n",
       "      <td>4.000000e+06</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>1.200000e+08</td>\n",
       "      <td>2.132624e+04</td>\n",
       "      <td>1.213090e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Init Fwd Win Byts  Down/Up Ratio  URG Flag Cnt      Dst Port  \\\n",
       "count       8.192732e+06   8.192732e+06  8.192732e+06  8.192732e+06   \n",
       "mean        1.236113e+04   4.586415e-01  4.536545e-02  1.079241e+04   \n",
       "std         1.979651e+04   7.159337e-01  2.081044e-01  2.014743e+04   \n",
       "min        -1.000000e+00   0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "25%         1.800000e+02   0.000000e+00  0.000000e+00  5.300000e+01   \n",
       "50%         2.052000e+03   0.000000e+00  0.000000e+00  8.000000e+01   \n",
       "75%         8.192000e+03   1.000000e+00  0.000000e+00  3.389000e+03   \n",
       "max         6.553500e+04   2.370000e+02  1.000000e+00  6.553500e+04   \n",
       "\n",
       "       Fwd Pkt Len Min  Fwd PSH Flags  Init Bwd Win Byts   Pkt Len Var  \\\n",
       "count     8.192732e+06   8.192732e+06       8.192732e+06  8.192732e+06   \n",
       "mean      9.020077e+00   2.921431e-02       7.876900e+03  4.045124e+04   \n",
       "std       2.407408e+01   1.684068e-01       1.924684e+04  2.063026e+05   \n",
       "min       0.000000e+00   0.000000e+00      -1.000000e+00  0.000000e+00   \n",
       "25%       0.000000e+00   0.000000e+00      -1.000000e+00  0.000000e+00   \n",
       "50%       0.000000e+00   0.000000e+00       0.000000e+00  7.363300e+02   \n",
       "75%       0.000000e+00   0.000000e+00       2.300000e+02  8.452794e+04   \n",
       "max       1.460000e+03   1.000000e+00       6.553500e+04  5.190000e+08   \n",
       "\n",
       "       RST Flag Cnt  Fwd Seg Size Min  ...  Bwd Pkt Len Min    Active Std  \\\n",
       "count  8.192732e+06      8.192732e+06  ...     8.192732e+06  8.192732e+06   \n",
       "mean   1.654723e-01      1.985582e+01  ...     2.084202e+01  4.098663e+04   \n",
       "std    3.716063e-01      8.445891e+00  ...     4.691458e+01  8.598273e+05   \n",
       "min    0.000000e+00      0.000000e+00  ...     0.000000e+00  0.000000e+00   \n",
       "25%    0.000000e+00      2.000000e+01  ...     0.000000e+00  0.000000e+00   \n",
       "50%    0.000000e+00      2.000000e+01  ...     0.000000e+00  0.000000e+00   \n",
       "75%    0.000000e+00      2.000000e+01  ...     0.000000e+00  0.000000e+00   \n",
       "max    1.000000e+00      5.600000e+01  ...     1.460000e+03  7.495336e+07   \n",
       "\n",
       "           Idle Min   Flow Pkts/s  PSH Flag Cnt  Flow Duration  Fwd IAT Mean  \\\n",
       "count  8.192732e+06  8.192732e+06  8.192732e+06   8.192732e+06  8.192732e+06   \n",
       "mean   4.348001e+06  7.123932e+04  3.934636e-01   1.009598e+07  3.772823e+06   \n",
       "std    8.831305e+07  2.995297e+05  4.885182e-01   6.946322e+08  3.136042e+08   \n",
       "min    0.000000e+00 -1.000000e-02  0.000000e+00  -9.190110e+11 -8.282200e+11   \n",
       "25%    0.000000e+00  5.720000e+00  0.000000e+00   5.280000e+02  0.000000e+00   \n",
       "50%    0.000000e+00  2.555300e+02  0.000000e+00   1.313600e+04  4.028500e+03   \n",
       "75%    0.000000e+00  5.511810e+03  1.000000e+00   2.567321e+06  2.945797e+05   \n",
       "max    2.399340e+11  4.000000e+06  1.000000e+00   1.200000e+08  1.200000e+08   \n",
       "\n",
       "       Bwd IAT Mean  Bwd Pkt Len Std  Tot Fwd Pkts  \n",
       "count  8.192732e+06     8.192732e+06  8.192732e+06  \n",
       "mean   5.547543e+05     1.210698e+02  5.328744e+00  \n",
       "std    2.828583e+06     1.894860e+02  7.954374e+01  \n",
       "min    0.000000e+00     0.000000e+00  1.000000e+00  \n",
       "25%    0.000000e+00     0.000000e+00  1.000000e+00  \n",
       "50%    0.000000e+00     0.000000e+00  2.000000e+00  \n",
       "75%    2.623100e+04     2.090000e+02  5.000000e+00  \n",
       "max    1.200000e+08     2.132624e+04  1.213090e+05  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61a55b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "# 3ï¸âƒ£ Scale features â€” very important for IF & LOF\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2315b1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Ratio: 0.2582\n"
     ]
    }
   ],
   "source": [
    "anomaly_ratio = df[df['Label'] != 'Benign'].shape[0] / df.shape[0]\n",
    "print(f\"Anomaly Ratio: {anomaly_ratio:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b94d9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4ï¸âƒ£ Initialize models\n",
    "models = {\n",
    "    \"Isolation Forest\": IsolationForest(\n",
    "        n_estimators=100,\n",
    "        contamination=anomaly_ratio,   # automatically estimate contamination\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cafaf78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = models['Isolation Forest']\n",
    "iso_forest.fit(X_scaled)\n",
    "\n",
    "# Predict anomalies: -1 = anomaly, 1 = normal\n",
    "y_pred_iso = iso_forest.predict(X_scaled)\n",
    "\n",
    "# Convert to 0 = normal, 1 = anomaly\n",
    "y_pred_iso_bin = (y_pred_iso == -1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62c9655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Isolation Forest ===\n",
      "Accuracy: 0.5508716750407556\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.70      0.70   6077145\n",
      "           1       0.13      0.13      0.13   2115587\n",
      "\n",
      "    accuracy                           0.55   8192732\n",
      "   macro avg       0.41      0.41      0.41   8192732\n",
      "weighted avg       0.55      0.55      0.55   8192732\n",
      "\n",
      "[[4239885 1837260]\n",
      " [1842328  273259]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "if y is not None:\n",
    "    # Convert labels to 0 (normal), 1 (anomaly) if not already\n",
    "    y_bin = (y != 'Benign').astype(int)  # Example: label 'Benign' = 0, attacks = 1\n",
    "\n",
    "    print(\"=== Isolation Forest ===\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_bin, y_pred_iso_bin))\n",
    "    print(classification_report(y_bin, y_pred_iso_bin))\n",
    "    print(confusion_matrix(y_bin, y_pred_iso_bin))\n",
    "\n",
    "    # print(\"\\n=== Local Outlier Factor ===\")\n",
    "    # print(\"Accuracy:\", accuracy_score(y_bin, y_pred_lof_bin))\n",
    "    # print(classification_report(y_bin, y_pred_lof_bin))\n",
    "    # print(confusion_matrix(y_bin, y_pred_lof_bin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c65075",
   "metadata": {},
   "source": [
    "##### Watchdog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0ad5b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a training set with 6077145 benign samples.\n"
     ]
    }
   ],
   "source": [
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# --- 2. Create the Benign-Only Training Set ---\n",
    "# The model will learn what is \"normal\" from this data exclusively.\n",
    "X_train_benign = X[y == 'Benign']\n",
    "print(f\"Created a training set with {len(X_train_benign)} benign samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c49558bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Scale the Benign Data ---\n",
    "# The scaler learns the statistical properties of ONLY the normal data.\n",
    "scaler = StandardScaler()\n",
    "X_train_benign_scaled = scaler.fit_transform(X_train_benign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be221cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Initialize and Train the Isolation Forest Model ---\n",
    "# 'contamination' is a sensitivity setting. Start with a low value.\n",
    "# This assumes ~1% of your benign data might have some noise or outlier-like features.\n",
    "# watchdog_model = IsolationForest(\n",
    "#     n_estimators=100,\n",
    "#     contamination=0.4,\n",
    "#     random_state=42,\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "watchdog_model = IsolationForest( #current best\n",
    "    n_estimators=200,      # More trees\n",
    "    max_samples=0.5,       # Use 50% of the data for each tree\n",
    "    contamination=0.03,    # Your calibrated sensitivity\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e153b116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Isolation Forest model on benign data...\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Train the model. It is now an expert on \"normal\" traffic.\n",
    "print(\"Training the Isolation Forest model on benign data...\")\n",
    "watchdog_model.fit(X_train_benign_scaled)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "30fb0b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scaler have been saved to 'watchdog_model.joblib' and 'scaler_watchdog.joblib'.\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Save the Trained Model and Scaler for Later Use ---\n",
    "# These files are what you'll use for live prediction (inference).\n",
    "joblib.dump(scaler, 'scaler_watchdog.joblib')\n",
    "joblib.dump(watchdog_model, 'watchdog_model.joblib')\n",
    "print(\"Model and scaler have been saved to 'watchdog_model.joblib' and 'scaler_watchdog.joblib'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3b274616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a test set with Benign traffic and 'SSH-Bruteforce' attacks.\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Prepare the Evaluation Data ---\n",
    "# Choose an attack type to act as the \"unknown\" threat for this test.\n",
    "# You can change this string to test against other attacks.\n",
    "UNKNOWN_ATTACK_TYPE = 'SSH-Bruteforce'\n",
    "print(f\"Creating a test set with Benign traffic and '{UNKNOWN_ATTACK_TYPE}' attacks.\")\n",
    "\n",
    "# Create the test set with a mix of normal and the chosen \"unknown\" attack\n",
    "X_test = X[(y == 'Benign') | (y == UNKNOWN_ATTACK_TYPE)]\n",
    "y_true_labels = y[(y == 'Benign') | (y == UNKNOWN_ATTACK_TYPE)]\n",
    "\n",
    "# Create binary labels for scoring (0 = Benign, 1 = Anomaly)\n",
    "y_true_bin = (y_true_labels != 'Benign').astype(int)\n",
    "\n",
    "# IMPORTANT: Scale the test data using the scaler that was fit on the training data\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47bfeeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making predictions on the test set...\n",
      "\n",
      "âœ… === Watchdog Model Evaluation Results === âœ…\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "         Benign (Normal)       1.00      0.97      0.98   6077145\n",
      "SSH-Bruteforce (Anomaly)       0.50      0.98      0.66    187589\n",
      "\n",
      "                accuracy                           0.97   6264734\n",
      "               macro avg       0.75      0.97      0.82   6264734\n",
      "            weighted avg       0.98      0.97      0.97   6264734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5894830  182315]\n",
      " [   4610  182979]]\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Make Predictions and Evaluate ---\n",
    "print(\"Making predictions on the test set...\")\n",
    "# Predict using the loaded model (-1 = anomaly, 1 = normal)\n",
    "y_pred = watchdog_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to our binary format (1 = anomaly, 0 = normal)\n",
    "y_pred_bin = (y_pred == -1).astype(int)\n",
    "\n",
    "# --- 5. Analyze the Results ---\n",
    "print(\"\\nâœ… === Watchdog Model Evaluation Results === âœ…\")\n",
    "print(classification_report(\n",
    "    y_true_bin,\n",
    "    y_pred_bin,\n",
    "    target_names=['Benign (Normal)', f'{UNKNOWN_ATTACK_TYPE} (Anomaly)'])\n",
    ")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fc890f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved watchdog model and scaler...\n",
      "Assets loaded successfully.\n",
      "Grouping all attack types into a single 'Anomaly' class...\n",
      "Making predictions on the full dataset...\n",
      "\n",
      "âœ… === Comprehensive Watchdog Evaluation Results === âœ…\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      " Benign (Class 0)       0.82      0.97      0.89   6077145\n",
      "Anomaly (Class 1)       0.81      0.37      0.51   2115587\n",
      "\n",
      "         accuracy                           0.82   8192732\n",
      "        macro avg       0.81      0.67      0.70   8192732\n",
      "     weighted avg       0.82      0.82      0.79   8192732\n",
      "\n",
      "Confusion Matrix:\n",
      "[[5894830  182315]\n",
      " [1327491  788096]]\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Load the Trained Model and Scaler ---\n",
    "# Make sure you have already run the training script.\n",
    "print(\"Loading saved watchdog model and scaler...\")\n",
    "watchdog_model = joblib.load('watchdog_model.joblib')\n",
    "scaler = joblib.load('scaler_watchdog.joblib')\n",
    "print(\"Assets loaded successfully.\")\n",
    "# --- 3. Prepare the Binary Labels (Benign vs. Anomaly) ---\n",
    "# This is the key step you suggested.\n",
    "# We create an \"answer key\" where anything that is NOT 'Benign' is an Anomaly (1).\n",
    "print(\"Grouping all attack types into a single 'Anomaly' class...\")\n",
    "y_true_bin = (y != 'Benign').astype(int)\n",
    "\n",
    "# --- 4. Scale the Entire Feature Set ---\n",
    "# We apply the scaler that was trained ONLY on benign data to the whole dataset.\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# --- 5. Make Predictions and Evaluate ---\n",
    "print(\"Making predictions on the full dataset...\")\n",
    "y_pred = watchdog_model.predict(X_scaled)\n",
    "\n",
    "# Convert predictions to our binary format (1 for anomaly, 0 for normal)\n",
    "y_pred_bin = (y_pred == -1).astype(int)\n",
    "\n",
    "# --- 6. Analyze the Overall Results ---\n",
    "print(\"\\nâœ… === Comprehensive Watchdog Evaluation Results === âœ…\")\n",
    "print(classification_report(\n",
    "    y_true_bin,\n",
    "    y_pred_bin,\n",
    "    target_names=['Benign (Class 0)', 'Anomaly (Class 1)'])\n",
    ")\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true_bin, y_pred_bin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8029768",
   "metadata": {},
   "source": [
    "##### More Hyperparamter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e77f5a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_samples': [256, 512, 0.5, 0.7],\n",
    "    'contamination': [0.02, 0.03, 0.04, 0.05],\n",
    "    'bootstrap': [True, False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2f275449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing combination 1: {'n_estimators': np.int64(200), 'max_samples': np.float64(0.5), 'contamination': np.float64(0.03), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.3108\n",
      "\n",
      "Testing combination 2: {'n_estimators': np.int64(300), 'max_samples': 512, 'contamination': np.float64(0.05), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.0119\n",
      "\n",
      "Testing combination 3: {'n_estimators': np.int64(300), 'max_samples': np.float64(0.5), 'contamination': np.float64(0.05), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.4083\n",
      "\n",
      "Testing combination 4: {'n_estimators': np.int64(100), 'max_samples': 256, 'contamination': np.float64(0.02), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.0026\n",
      "\n",
      "Testing combination 5: {'n_estimators': np.int64(300), 'max_samples': 512, 'contamination': np.float64(0.03), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.0048\n",
      "\n",
      "Testing combination 6: {'n_estimators': np.int64(200), 'max_samples': 512, 'contamination': np.float64(0.02), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.0029\n",
      "\n",
      "Testing combination 7: {'n_estimators': np.int64(200), 'max_samples': np.float64(0.5), 'contamination': np.float64(0.02), 'bootstrap': np.False_}\n",
      "Anomaly Recall for this combo: 0.2712\n",
      "\n",
      "Testing combination 8: {'n_estimators': np.int64(100), 'max_samples': np.float64(0.7), 'contamination': np.float64(0.04), 'bootstrap': np.False_}\n",
      "Anomaly Recall for this combo: 0.3455\n",
      "\n",
      "Testing combination 9: {'n_estimators': np.int64(100), 'max_samples': 512, 'contamination': np.float64(0.03), 'bootstrap': np.True_}\n",
      "Anomaly Recall for this combo: 0.0049\n",
      "\n",
      "Testing combination 10: {'n_estimators': np.int64(300), 'max_samples': 256, 'contamination': np.float64(0.05), 'bootstrap': np.False_}\n",
      "Anomaly Recall for this combo: 0.0070\n",
      "\n",
      "--- Search Complete ---\n",
      "Best Anomaly Recall found: 0.4083\n",
      "Best Hyperparameters: {'n_estimators': np.int64(300), 'max_samples': np.float64(0.5), 'contamination': np.float64(0.05), 'bootstrap': np.True_}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, recall_score\n",
    "import numpy as np\n",
    "X = df.drop(columns=['Label'])\n",
    "y = (df['Label'] != 'Benign').astype(int) # 0 for Benign, 1 for Anomaly\n",
    "\n",
    "# a) Create the benign-only training data\n",
    "X_train_benign = X[y == 0]\n",
    "\n",
    "# b) Create a separate validation set to score the models\n",
    "X_temp, X_validation, y_temp, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# --- 2. Scale the Data ---\n",
    "scaler = StandardScaler()\n",
    "X_train_benign_scaled = scaler.fit_transform(X_train_benign)\n",
    "X_validation_scaled = scaler.transform(X_validation)\n",
    "\n",
    "# --- 3. Set Up and Run the Search ---\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_samples': [256, 512, 0.5, 0.7],\n",
    "    'contamination': [0.02, 0.03, 0.04, 0.05],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "\n",
    "# Test 10 random combinations\n",
    "for i in range(10): \n",
    "    params = {k: np.random.choice(v) for k, v in param_dist.items()}\n",
    "    \n",
    "    # FIX: Correct the data type for the 'max_samples' parameter\n",
    "    if params['max_samples'] > 1.0:\n",
    "        params['max_samples'] = int(params['max_samples'])\n",
    "            \n",
    "    print(f\"\\nTesting combination {i+1}: {params}\")\n",
    "    \n",
    "    model = IsolationForest(**params, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Train on benign data only\n",
    "    model.fit(X_train_benign_scaled)\n",
    "    \n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_validation_scaled)\n",
    "    y_pred_bin = (y_pred == -1).astype(int)\n",
    "    \n",
    "    # Calculate score\n",
    "    score = recall_score(y_validation, y_pred_bin, pos_label=1)\n",
    "    print(f\"Anomaly Recall for this combo: {score:.4f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params\n",
    "\n",
    "print(\"\\n--- Search Complete ---\")\n",
    "print(f\"Best Anomaly Recall found: {best_score:.4f}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618ce22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
