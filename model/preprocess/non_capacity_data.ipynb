{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa53c0ed",
   "metadata": {},
   "source": [
    "#### Tried this pre-process technique, possible to use. But my laptop will explode due to it being not capable of handling and processing this data. This code isn't possible for my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "025fbf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b77f5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '../data/'\n",
    "CHUNK_SIZE = 50000  # Process data in chunks to manage memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a731be",
   "metadata": {},
   "source": [
    "##### Below cell combines all the files into a combined file and also Excluded_labels into single label named anomoly.\n",
    "This takes up a lot of space in C drive due to pandas temp storage. DON'T RUN IT AGAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39c901bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 CSV files\n",
      "Loading: 02-14-2018.csv\n",
      "Loading: 02-15-2018.csv\n",
      "Loading: 02-16-2018.csv\n",
      "Loading: 02-21-2018.csv\n",
      "Loading: 02-22-2018.csv\n",
      "Loading: 02-23-2018.csv\n",
      "Loading: 02-28-2018.csv\n",
      "Loading: 03-01-2018.csv\n",
      "Loading: 03-02-2018.csv\n",
      "Combined dataset shape: (8284195, 79)\n",
      "Combined raw-cleaned dataset saved to: ../data//combined/combined_raw.csv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = DATA_PATH+'/combined/combined_raw.csv'\n",
    "# Excluded / rare labels → will be merged as \"Anomoly\"\n",
    "EXCLUDE_LABELS = [\n",
    "    'DoS attacks-Slowloris',\n",
    "    'DoS attacks-GoldenEye',\n",
    "    'DDOS attack-LOIC-UDP',\n",
    "    'Brute Force -Web',\n",
    "    'Brute Force -XSS',\n",
    "    'SQL Injection'\n",
    "]\n",
    "\n",
    "def get_file_paths(data_path):\n",
    "    \"\"\"Get all CSV file paths except the corrupted one\"\"\"\n",
    "    file_paths = []\n",
    "    for root, _, filenames in os.walk(data_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_paths.append(os.path.join(root, filename))\n",
    "    # remove the corrupted file\n",
    "    bad_file = os.path.join(DATA_PATH, '02-20-2018.csv')\n",
    "    if bad_file in file_paths:\n",
    "        file_paths.remove(bad_file)\n",
    "    return sorted(file_paths)\n",
    "\n",
    "# Get file paths\n",
    "file_paths = get_file_paths(DATA_PATH)\n",
    "print(f\"Found {len(file_paths)} CSV files\")\n",
    "\n",
    "# Read + clean + combine\n",
    "df_list = []\n",
    "for file_path in file_paths:\n",
    "    print(f\"Loading: {os.path.basename(file_path)}\")\n",
    "    chunk = pd.read_csv(file_path, low_memory=False)\n",
    "    \n",
    "    # Drop timestamp if exists\n",
    "    chunk = chunk.drop(columns=['Timestamp'], errors='ignore')\n",
    "    \n",
    "    # Remove header rows accidentally read as data\n",
    "    chunk = chunk[chunk['Label'] != 'Label']\n",
    "    \n",
    "    # Map excluded labels to \"Anomoly\"\n",
    "    chunk['Label'] = chunk['Label'].apply(\n",
    "        lambda x: 'Anomoly' if x in EXCLUDE_LABELS else x\n",
    "    )\n",
    "    \n",
    "    df_list.append(chunk)\n",
    "\n",
    "# Combine all dataframes\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(OUTPUT_PATH, index=False)\n",
    "print(f\"Combined raw-cleaned dataset saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ee5b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dst Port</th>\n",
       "      <th>Protocol</th>\n",
       "      <th>Flow Duration</th>\n",
       "      <th>Tot Fwd Pkts</th>\n",
       "      <th>Tot Bwd Pkts</th>\n",
       "      <th>TotLen Fwd Pkts</th>\n",
       "      <th>TotLen Bwd Pkts</th>\n",
       "      <th>Fwd Pkt Len Max</th>\n",
       "      <th>Fwd Pkt Len Min</th>\n",
       "      <th>Fwd Pkt Len Mean</th>\n",
       "      <th>...</th>\n",
       "      <th>Fwd Seg Size Min</th>\n",
       "      <th>Active Mean</th>\n",
       "      <th>Active Std</th>\n",
       "      <th>Active Max</th>\n",
       "      <th>Active Min</th>\n",
       "      <th>Idle Mean</th>\n",
       "      <th>Idle Std</th>\n",
       "      <th>Idle Max</th>\n",
       "      <th>Idle Min</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112641719</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56320859.5</td>\n",
       "      <td>139.300036</td>\n",
       "      <td>56320958.0</td>\n",
       "      <td>56320761.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112641466</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56320733.0</td>\n",
       "      <td>114.551299</td>\n",
       "      <td>56320814.0</td>\n",
       "      <td>56320652.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112638623</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56319311.5</td>\n",
       "      <td>301.934596</td>\n",
       "      <td>56319525.0</td>\n",
       "      <td>56319098.0</td>\n",
       "      <td>Benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 79 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Dst Port  Protocol  Flow Duration  Tot Fwd Pkts  Tot Bwd Pkts  \\\n",
       "0         0         0      112641719             3             0   \n",
       "1         0         0      112641466             3             0   \n",
       "2         0         0      112638623             3             0   \n",
       "\n",
       "   TotLen Fwd Pkts  TotLen Bwd Pkts  Fwd Pkt Len Max  Fwd Pkt Len Min  \\\n",
       "0                0              0.0                0                0   \n",
       "1                0              0.0                0                0   \n",
       "2                0              0.0                0                0   \n",
       "\n",
       "   Fwd Pkt Len Mean  ...  Fwd Seg Size Min  Active Mean  Active Std  \\\n",
       "0               0.0  ...                 0          0.0         0.0   \n",
       "1               0.0  ...                 0          0.0         0.0   \n",
       "2               0.0  ...                 0          0.0         0.0   \n",
       "\n",
       "   Active Max  Active Min   Idle Mean    Idle Std    Idle Max    Idle Min  \\\n",
       "0         0.0         0.0  56320859.5  139.300036  56320958.0  56320761.0   \n",
       "1         0.0         0.0  56320733.0  114.551299  56320814.0  56320652.0   \n",
       "2         0.0         0.0  56319311.5  301.934596  56319525.0  56319098.0   \n",
       "\n",
       "    Label  \n",
       "0  Benign  \n",
       "1  Benign  \n",
       "2  Benign  \n",
       "\n",
       "[3 rows x 79 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('..\\data\\combined\\combined_raw.csv')\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0ccbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8284195 entries, 0 to 8284194\n",
      "Data columns (total 79 columns):\n",
      " #   Column             Dtype  \n",
      "---  ------             -----  \n",
      " 0   Dst Port           int64  \n",
      " 1   Protocol           int64  \n",
      " 2   Flow Duration      int64  \n",
      " 3   Tot Fwd Pkts       int64  \n",
      " 4   Tot Bwd Pkts       int64  \n",
      " 5   TotLen Fwd Pkts    int64  \n",
      " 6   TotLen Bwd Pkts    float64\n",
      " 7   Fwd Pkt Len Max    int64  \n",
      " 8   Fwd Pkt Len Min    int64  \n",
      " 9   Fwd Pkt Len Mean   float64\n",
      " 10  Fwd Pkt Len Std    float64\n",
      " 11  Bwd Pkt Len Max    int64  \n",
      " 12  Bwd Pkt Len Min    int64  \n",
      " 13  Bwd Pkt Len Mean   float64\n",
      " 14  Bwd Pkt Len Std    float64\n",
      " 15  Flow Byts/s        float64\n",
      " 16  Flow Pkts/s        float64\n",
      " 17  Flow IAT Mean      float64\n",
      " 18  Flow IAT Std       float64\n",
      " 19  Flow IAT Max       float64\n",
      " 20  Flow IAT Min       float64\n",
      " 21  Fwd IAT Tot        float64\n",
      " 22  Fwd IAT Mean       float64\n",
      " 23  Fwd IAT Std        float64\n",
      " 24  Fwd IAT Max        float64\n",
      " 25  Fwd IAT Min        float64\n",
      " 26  Bwd IAT Tot        float64\n",
      " 27  Bwd IAT Mean       float64\n",
      " 28  Bwd IAT Std        float64\n",
      " 29  Bwd IAT Max        float64\n",
      " 30  Bwd IAT Min        float64\n",
      " 31  Fwd PSH Flags      int64  \n",
      " 32  Bwd PSH Flags      int64  \n",
      " 33  Fwd URG Flags      int64  \n",
      " 34  Bwd URG Flags      int64  \n",
      " 35  Fwd Header Len     int64  \n",
      " 36  Bwd Header Len     int64  \n",
      " 37  Fwd Pkts/s         float64\n",
      " 38  Bwd Pkts/s         float64\n",
      " 39  Pkt Len Min        int64  \n",
      " 40  Pkt Len Max        int64  \n",
      " 41  Pkt Len Mean       float64\n",
      " 42  Pkt Len Std        float64\n",
      " 43  Pkt Len Var        float64\n",
      " 44  FIN Flag Cnt       int64  \n",
      " 45  SYN Flag Cnt       int64  \n",
      " 46  RST Flag Cnt       int64  \n",
      " 47  PSH Flag Cnt       int64  \n",
      " 48  ACK Flag Cnt       int64  \n",
      " 49  URG Flag Cnt       int64  \n",
      " 50  CWE Flag Count     int64  \n",
      " 51  ECE Flag Cnt       int64  \n",
      " 52  Down/Up Ratio      int64  \n",
      " 53  Pkt Size Avg       float64\n",
      " 54  Fwd Seg Size Avg   float64\n",
      " 55  Bwd Seg Size Avg   float64\n",
      " 56  Fwd Byts/b Avg     int64  \n",
      " 57  Fwd Pkts/b Avg     int64  \n",
      " 58  Fwd Blk Rate Avg   int64  \n",
      " 59  Bwd Byts/b Avg     int64  \n",
      " 60  Bwd Pkts/b Avg     int64  \n",
      " 61  Bwd Blk Rate Avg   int64  \n",
      " 62  Subflow Fwd Pkts   int64  \n",
      " 63  Subflow Fwd Byts   int64  \n",
      " 64  Subflow Bwd Pkts   int64  \n",
      " 65  Subflow Bwd Byts   int64  \n",
      " 66  Init Fwd Win Byts  int64  \n",
      " 67  Init Bwd Win Byts  int64  \n",
      " 68  Fwd Act Data Pkts  int64  \n",
      " 69  Fwd Seg Size Min   int64  \n",
      " 70  Active Mean        float64\n",
      " 71  Active Std         float64\n",
      " 72  Active Max         float64\n",
      " 73  Active Min         float64\n",
      " 74  Idle Mean          float64\n",
      " 75  Idle Std           float64\n",
      " 76  Idle Max           float64\n",
      " 77  Idle Min           float64\n",
      " 78  Label              object \n",
      "dtypes: float64(37), int64(41), object(1)\n",
      "memory usage: 4.9+ GB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb36c57",
   "metadata": {},
   "source": [
    "#### Combining and scaling all the values into single csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaad489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\VII\\ANT\\preprocess\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f78a0623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9 CSV files\n",
      "Loading and cleaning data...\n",
      "First pass: Collecting unique labels...\n",
      "Scanning: 02-14-2018.csv\n",
      "Scanning: 02-15-2018.csv\n",
      "Scanning: 02-16-2018.csv\n",
      "Scanning: 02-21-2018.csv\n",
      "Scanning: 02-22-2018.csv\n",
      "Scanning: 02-23-2018.csv\n",
      "Scanning: 02-28-2018.csv\n",
      "Scanning: 03-01-2018.csv\n",
      "Scanning: 03-02-2018.csv\n",
      "['FTP-BruteForce', 'DoS attacks-SlowHTTPTest', 'Infilteration', 'DDOS attack-HOIC', 'Benign', 'DoS attacks-Hulk', 'Bot', 'SSH-Bruteforce', 'Anomoly']\n",
      "Found 9 unique attack types\n",
      "Processing: 02-14-2018.csv\n",
      "Processing: 02-15-2018.csv\n",
      "Processing: 02-16-2018.csv\n",
      "Processing: 02-21-2018.csv\n",
      "Processing: 02-22-2018.csv\n",
      "Processing: 02-23-2018.csv\n",
      "Processing: 02-28-2018.csv\n",
      "Processing: 03-01-2018.csv\n",
      "Processing: 03-02-2018.csv\n",
      "Combining all processed chunks...\n"
     ]
    }
   ],
   "source": [
    "# Optimized function to get all CSV files\n",
    "def get_file_paths(data_path):\n",
    "    \"\"\"Get all CSV file paths efficiently\"\"\"\n",
    "    file_paths = []\n",
    "    \n",
    "    for root, _, filenames in os.walk(data_path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.csv'):\n",
    "                file_paths.append(os.path.join(root, filename))\n",
    "    file_paths.remove(DATA_PATH+'02-20-2018.csv')\n",
    "    print(f\"Found {len(file_paths)} CSV files\")\n",
    "    return sorted(file_paths)\n",
    "\n",
    "# Optimized data loading function\n",
    "def load_and_clean_data(file_paths):\n",
    "    \"\"\"Load and clean data from multiple CSV files efficiently\"\"\"\n",
    "    # Labels to exclude (too few samples, sum as 'Anomoly')\n",
    "    exclude_labels = ['DoS attacks-Slowloris', 'DoS attacks-GoldenEye', 'DDOS attack-LOIC-UDP', \n",
    "                     'Brute Force -Web', 'Brute Force -XSS', 'SQL Injection']\n",
    "    \n",
    "    # Initialize label encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    all_labels = []\n",
    "    \n",
    "    # First pass: collect all unique labels\n",
    "    print(\"First pass: Collecting unique labels...\")\n",
    "    for file_path in file_paths:\n",
    "        print(f\"Scanning: {os.path.basename(file_path)}\")\n",
    "        # Read only the Label column to save memory\n",
    "        labels = pd.read_csv(file_path, usecols=['Label'], low_memory=False)\n",
    "        all_labels.extend(labels['Label'].unique())\n",
    "    \n",
    "    # Get unique labels and fit encoder\n",
    "    unique_labels = list(set(all_labels) - set(exclude_labels))\n",
    "    \n",
    "    if 'Label' in unique_labels:\n",
    "        unique_labels.remove('Label')\n",
    "        \n",
    "    excluded_found = any(label in all_labels for label in exclude_labels)\n",
    "    if excluded_found:\n",
    "        unique_labels.append('Anomoly')\n",
    "        \n",
    "    print(unique_labels)\n",
    "    label_encoder.fit(unique_labels)\n",
    "    print(f\"Found {len(unique_labels)} unique attack types\")\n",
    "    \n",
    "    # Second pass: load and process data\n",
    "    processed_chunks = []\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        print(f\"Processing: {os.path.basename(file_path)}\")\n",
    "        \n",
    "        # Read in chunks to manage memory\n",
    "        chunk_reader = pd.read_csv(file_path, chunksize=CHUNK_SIZE, low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_reader:\n",
    "            # Remove timestamp and excluded labels\n",
    "            chunk = chunk.drop(columns=['Timestamp'], errors='ignore')\n",
    "            \n",
    "            if len(chunk) > 0:\n",
    "                \n",
    "                chunk = chunk[chunk['Label'] != 'Label']\n",
    "                chunk['Label'] = chunk['Label'].apply(lambda x: 'Anomoly' if x in exclude_labels else x)\n",
    "                \n",
    "                # Encode labels\n",
    "                chunk['Label'] = label_encoder.transform(chunk['Label'])\n",
    "                \n",
    "                # Convert to numeric and clean\n",
    "                chunk = chunk.apply(pd.to_numeric, errors='coerce')\n",
    "                chunk = chunk.replace([np.inf, -np.inf], np.nan)\n",
    "                chunk = chunk.dropna()\n",
    "                \n",
    "                processed_chunks.append(chunk)\n",
    "\n",
    "                if len(processed_chunks) % 10 == 0:  # Every 10 chunks\n",
    "                    gc.collect()\n",
    "        \n",
    "        # Clear memory\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    print(\"Combining all processed chunks...\")\n",
    "    combined_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "    del processed_chunks\n",
    "    gc.collect()\n",
    "    \n",
    "    return combined_df, label_encoder\n",
    "\n",
    "# Get file paths\n",
    "file_paths = get_file_paths(DATA_PATH)\n",
    "\n",
    "# Load and clean data\n",
    "print(\"Loading and cleaning data...\")\n",
    "df, label_encoder_fitted = load_and_clean_data(file_paths)\n",
    "\n",
    "_=gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "829e8e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoder mapping:\n",
      "0: Anomoly\n",
      "1: Benign\n",
      "2: Bot\n",
      "3: DDOS attack-HOIC\n",
      "4: DoS attacks-Hulk\n",
      "5: DoS attacks-SlowHTTPTest\n",
      "6: FTP-BruteForce\n",
      "7: Infilteration\n",
      "8: SSH-Bruteforce\n"
     ]
    }
   ],
   "source": [
    "print(\"Label encoder mapping:\")\n",
    "for i, label in enumerate(label_encoder_fitted.classes_):\n",
    "    print(f\"{i}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b864b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving as CSV...\n",
      "Mapping saved as ../data/data.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving as CSV...\")\n",
    "mapping_df = pd.DataFrame({\n",
    "    'encoded_value': range(len(label_encoder_fitted.classes_)),\n",
    "    'label_name': label_encoder_fitted.classes_\n",
    "})\n",
    "mapping_df.to_csv(DATA_PATH+'label_mapping.csv', index=False)\n",
    "print(\"Mapping saved as \"+DATA_PATH+'label_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e742518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset shape: (8247888, 79)\n",
      "Number of classes: 9\n",
      "Label distribution:\n",
      "Label\n",
      "0      55156\n",
      "1    6077145\n",
      "2     286191\n",
      "3     686012\n",
      "4     461912\n",
      "5     139890\n",
      "6     193354\n",
      "7     160639\n",
      "8     187589\n",
      "Name: count, dtype: int64\n",
      "Memory usage: 4971.19 MB\n"
     ]
    }
   ],
   "source": [
    "# Display data information\n",
    "print(f\"Total dataset shape: {df.shape}\")\n",
    "print(f\"Number of classes: {df['Label'].nunique()}\")\n",
    "print(\"Label distribution:\")\n",
    "print(df['Label'].value_counts().sort_index())\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6133a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self, label_encoder=None):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = label_encoder\n",
    "        self.feature_columns = None\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, df):\n",
    "        \"\"\"Fit preprocessor on training data\"\"\"\n",
    "        # Identify feature columns (exclude Label)\n",
    "        self.feature_columns = [col for col in df.columns if col != 'Label']\n",
    "        \n",
    "        # Fit scaler on numeric features\n",
    "        self.scaler.fit(df[self.feature_columns])\n",
    "        self.is_fitted = True\n",
    "        \n",
    "    def transform(self, df):\n",
    "        \"\"\"Transform data\"\"\"\n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Preprocessor must be fitted before transform\")\n",
    "            \n",
    "        # Select features\n",
    "        X = df[self.feature_columns]\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        df_scaled = df.copy()\n",
    "        df_scaled[self.feature_columns] = X_scaled\n",
    "        \n",
    "        return df_scaled\n",
    "    \n",
    "# Initialize preprocessor with fitted label encoder\n",
    "preprocessor = DataPreprocessor(label_encoder=label_encoder_fitted)\n",
    "\n",
    "# Fit on data\n",
    "preprocessor.fit(df)\n",
    "\n",
    "# Transform data\n",
    "df_scaled = preprocessor.transform(df)\n",
    "X = df_scaled.drop('Label', axis=1)\n",
    "y = df_scaled['Label']\n",
    "\n",
    "print(f\"Scaled features shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Classes: {np.unique(y)}\")\n",
    "\n",
    "# Memory cleanup\n",
    "del X, y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4dbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data for faster future loading\n",
    "n_parts = 1\n",
    "part_size = len(df_scaled) // n_parts\n",
    "\n",
    "for i in range(n_parts):\n",
    "    start_idx = i * part_size\n",
    "    if i == n_parts - 1:  # Last part gets any remaining rows\n",
    "        end_idx = len(df_scaled)\n",
    "    else:\n",
    "        end_idx = (i + 1) * part_size\n",
    "    \n",
    "    part = df_scaled.iloc[start_idx:end_idx]\n",
    "    save_path = f'/kaggle/working/data_part_{i+1}.csv'\n",
    "    part.to_csv(save_path, index=False)\n",
    "    print(f\"Cleaned data part {i+1} saved to: {save_path}\")\n",
    "\n",
    "    del part\n",
    "    gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
